{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fareesah28/INM705-CW/blob/main/MariumColabFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DINOv2 + Transformer**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8fayzjJrRYuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Before running the first code block, please *download the file from the following link and upload it to your Google Drive:**\n",
        "\n",
        "https://drive.google.com/file/d/145OHbfMAaot25bgx2tDRsiIXvOKLvezi/view?usp=sharing"
      ],
      "metadata": {
        "id": "a8ZJwtq3RWLs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4HXl5aNJuwD"
      },
      "source": [
        "# **Stage 1: Data Loading & Extraction**\n",
        "(Mount Drive, unzip dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_bq3LEuJCWe",
        "outputId": "6080ca2f-3212-4111-d2bc-727ab3ba1358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qv5dlmTJYrS"
      },
      "outputs": [],
      "source": [
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/drive/MyDrive/data_trimmed_clean.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 2: Dataset Restructuring**\n",
        "(Organize frames into folders per video)"
      ],
      "metadata": {
        "id": "iR-C_SN_EYiz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDp7GZV9ZZdp",
        "outputId": "abfeb16d-bf98-4083-8330-93c8b5dfdbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing class: Arrest\n",
            "Processing class: Burglary\n",
            "Processing class: NormalVideos\n",
            "Processing class: Shooting\n",
            "Processing class: Fighting\n",
            "Processing class: Arson\n",
            "Processing class: Assault\n",
            "Processing class: Explosion\n",
            "Done restructuring: /content/data_trimmed/Train → /content/data_trimmed_restructured/Train\n",
            "\n",
            "Processing class: Arrest\n",
            "Processing class: Burglary\n",
            "Processing class: NormalVideos\n",
            "Processing class: Shooting\n",
            "Processing class: Fighting\n",
            "Processing class: Arson\n",
            "Processing class: Assault\n",
            "Processing class: Explosion\n",
            "Done restructuring: /content/data_trimmed/Test → /content/data_trimmed_restructured/Test\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def extract_video_id(filename):\n",
        "    \"\"\"\n",
        "    Extracts the video ID by removing the frame suffix (assumes last underscore + digits is the frame number).\n",
        "    For example:\n",
        "        Normal_Videos_003_x264_0.png → Normal_Videos_003_x264\n",
        "        Assault_001_frame_010.png → Assault_001\n",
        "    \"\"\"\n",
        "    parts = filename.rsplit(\"_\", 1)\n",
        "    return parts[0] if len(parts) == 2 else filename.split(\"_frame\")[0]\n",
        "\n",
        "\n",
        "def restructure_dataset(src_dir, dst_dir):\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    for class_name in os.listdir(src_dir):\n",
        "        class_path = os.path.join(src_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "\n",
        "\n",
        "        print(f\"Processing class: {class_name}\")\n",
        "        video_frame_dict = defaultdict(list)\n",
        "\n",
        "\n",
        "        for fname in os.listdir(class_path):\n",
        "            if not fname.endswith('.png'):\n",
        "                continue\n",
        "\n",
        "\n",
        "            video_id = extract_video_id(fname)\n",
        "            video_frame_dict[video_id].append(fname)\n",
        "\n",
        "\n",
        "        for video_id, frames in video_frame_dict.items():\n",
        "            video_folder_path = os.path.join(dst_dir, class_name, video_id)\n",
        "            os.makedirs(video_folder_path, exist_ok=True)\n",
        "\n",
        "\n",
        "            for frame in frames:\n",
        "                src = os.path.join(class_path, frame)\n",
        "                dst = os.path.join(video_folder_path, frame)\n",
        "                shutil.copy2(src, dst)\n",
        "\n",
        "\n",
        "    print(f\"Done restructuring: {src_dir} → {dst_dir}\\n\")\n",
        "\n",
        "\n",
        "train_dir = \"/content/data_trimmed/Train\"\n",
        "test_dir = \"/content/data_trimmed/Test\"\n",
        "\n",
        "\n",
        "train_dst = \"/content/data_trimmed_restructured/Train\"\n",
        "test_dst = \"/content/data_trimmed_restructured/Test\"\n",
        "\n",
        "\n",
        "# Run restructuring\n",
        "restructure_dataset(train_dir, train_dst)\n",
        "restructure_dataset(test_dir, test_dst)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 3: Preprocessing & Dataset Creation**\n",
        "(Resize frames, normalize, pad sequences, build train/test datasets)"
      ],
      "metadata": {
        "id": "DDUHe4GIElAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIeYtDx4Za3K"
      },
      "outputs": [],
      "source": [
        "IMG_HEIGHT = 64\n",
        "IMG_WIDTH = 64\n",
        "SEQUENCE_LENGTH = 16\n",
        "BATCH_SIZE = 8\n",
        "CLASS_NAMES = ['Arrest','Arson','Assault','Burglary','Explosion','Fighting','NormalVideos','Shooting']\n",
        "NUM_CLASSES = len(CLASS_NAMES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Pyg22k-Ze4Y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from glob import glob\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def load_video_frames(video_dir, sequence_length, img_size):\n",
        "    # Get sorted list of frame paths\n",
        "    frame_paths = sorted(glob(os.path.join(video_dir, \"*.png\")))\n",
        "\n",
        "\n",
        "    frames = []\n",
        "    for path in frame_paths[:sequence_length]:\n",
        "        img = Image.open(path).resize(img_size)\n",
        "        frame = np.array(img).astype(\"float32\") / 255.0  # Normalize to [0, 1]\n",
        "        frames.append(frame)\n",
        "\n",
        "\n",
        "\n",
        "    while len(frames) < sequence_length:\n",
        "        frames.append(np.zeros((img_size[1], img_size[0], 3), dtype=\"float32\"))\n",
        "\n",
        "\n",
        "    return np.stack(frames)\n",
        "\n",
        "\n",
        "def get_video_paths_and_labels(base_dir, class_names):\n",
        "    video_paths = []\n",
        "    labels = []\n",
        "\n",
        "\n",
        "    for class_index, class_name in enumerate(class_names):\n",
        "        class_path = os.path.join(base_dir, class_name)\n",
        "        for video_folder in os.listdir(class_path):\n",
        "            video_path = os.path.join(class_path, video_folder)\n",
        "            if os.path.isdir(video_path):\n",
        "                video_paths.append(video_path)\n",
        "                labels.append(class_index)\n",
        "\n",
        "\n",
        "    return video_paths, labels\n",
        "\n",
        "\n",
        "def build_video_dataset(base_dir, sequence_length, img_size, batch_size, class_names, shuffle=True):\n",
        "    video_paths, labels = get_video_paths_and_labels(base_dir, class_names)\n",
        "\n",
        "\n",
        "    def generator():\n",
        "        for video_path, label in zip(video_paths, labels):\n",
        "            frames = load_video_frames(video_path, sequence_length, img_size)\n",
        "            yield frames, to_categorical(label, num_classes=len(class_names))\n",
        "\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(sequence_length, img_size[1], img_size[0], 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(len(class_names),), dtype=tf.float32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(buffer_size=len(video_paths))\n",
        "\n",
        "\n",
        "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdw8pXgOZoTx"
      },
      "outputs": [],
      "source": [
        "train_dir = \"data_trimmed_restructured/Train\"\n",
        "test_dir = \"data_trimmed_restructured/Test\"\n",
        "train_dataset = build_video_dataset(\n",
        "    base_dir=train_dir,\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    img_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_names=CLASS_NAMES,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "test_dataset = build_video_dataset(\n",
        "    base_dir=test_dir,\n",
        "    sequence_length=SEQUENCE_LENGTH,\n",
        "    img_size=(IMG_WIDTH, IMG_HEIGHT),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_names=CLASS_NAMES,\n",
        "    shuffle=False\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1MHUaUbZsEm",
        "outputId": "5a9bb155-dd8f-469a-99d5-5251ae82e9fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 16, 64, 64, 3)\n",
            "(8, 8)\n"
          ]
        }
      ],
      "source": [
        "for x, y in train_dataset.take(1):\n",
        "    print(x.shape)  # (8, 16, 64, 64, 3)\n",
        "    print(y.shape)  # (8, 8)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2AvVN5uaZuZc",
        "outputId": "6135e4ce-d648-4abb-cdfb-ccb4a55c4bcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 4: Feature Extraction with DINOv2**\n",
        "(Use DINOv2 to extract features from frames)"
      ],
      "metadata": {
        "id": "ezli3UprEu3n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OF6bgg4yaRFl"
      },
      "source": [
        "Basic Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "d5129a04689f43a8b0d400113f8f1a5e",
            "2d5da6f1334f4d77ba9e69b089cce178",
            "debc7d87b4104de2b5fb9984b637a70b",
            "0deb54dd23614de7afeed6ca5cb73bd5",
            "1cc740f67c9449b09926f51caf8b791d",
            "0bf3cfb98bab40cf871e706032b629ea",
            "9ff6ab368a7e44609b3b3843993ca3cf",
            "7d3b9372d2844b73a0f1dbc686946103",
            "21e19e717f724376a1b912b84917120e",
            "7808eeecb2ab441cb46d83ba6499c3f8",
            "3ad04e9c85d543eca664a861e0a79ed2",
            "580ac6e13534478bb9a8132450a225ca",
            "b2194986a52c4d8a88fccbf1da7500ad",
            "7ec600f23c5d460194e0df9308638ac9",
            "3c163a8e698a4b0a9ea6a9d19b4a2e94",
            "2098129901ed41beac8cc00d6a44d4ec",
            "cea5e40ddae145d3a72ac5be61ea5f34",
            "773d6bce56304e04acddd7457c2b08c5",
            "4d29f92bf0c34be7bf4a2256ccfc6a08",
            "e05bec7685614add914dadfa6ce51f15",
            "369168e857e343ddbfc403f76ef8ac3e",
            "7c734beb073b4e8fa12c545de7e6164e",
            "ef1a6ef21b964f6285e68d582703dafb",
            "2ea907ea17cb40adbcb71f4bb187a7f7",
            "049de2b16baa42d29e67f0aba40f3969",
            "e8068aa8484f4b49bb236823cef0a1b1",
            "0708d7f480a2455898c2d93018c97f45",
            "18b5fe694d264766aa2c953481e0314d",
            "b24b4db958224668a5db058f8789025f",
            "a13062537cd84ef08b809abe836e8a3b",
            "7d6210cb69314f80ada174be5d85feea",
            "9900debefe054250b04b1f4752b95141",
            "53f2ea261cc04ef3bf05c17898e0cf4e"
          ]
        },
        "id": "otL6WDSBZ3Cz",
        "outputId": "f736b475-637a-4f17-8d02-de6f370e4a81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/436 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5129a04689f43a8b0d400113f8f1a5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "580ac6e13534478bb9a8132450a225ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef1a6ef21b964f6285e68d582703dafb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "dinov2 = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device)\n",
        "dinov2.eval()\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, labels, sequence_length, img_size, class_names):\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.sequence_length = sequence_length\n",
        "        self.img_size = img_size\n",
        "        self.class_names = class_names\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "\n",
        "        frame_paths = sorted([\n",
        "            os.path.join(video_path, f)\n",
        "            for f in os.listdir(video_path)\n",
        "            if f.endswith(\".png\")\n",
        "        ])\n",
        "\n",
        "\n",
        "        frames = []\n",
        "        for path in frame_paths[:self.sequence_length]:\n",
        "            img = Image.open(path).resize(self.img_size)\n",
        "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                features = dinov2(**inputs).last_hidden_state.mean(dim=1).squeeze(0)\n",
        "            frames.append(features)\n",
        "\n",
        "\n",
        "        while len(frames) < self.sequence_length:\n",
        "            frames.append(torch.zeros_like(frames[0]))\n",
        "\n",
        "\n",
        "        sequence = torch.stack(frames)\n",
        "        return sequence, torch.tensor(label, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmD4TRjCZ7Os"
      },
      "outputs": [],
      "source": [
        "class VideoTransformerClassifier(nn.Module):\n",
        "    def __init__(self, feature_dim, num_classes, sequence_length, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, sequence_length, feature_dim))\n",
        "\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=feature_dim,\n",
        "                nhead=4,\n",
        "                dim_feedforward=512,\n",
        "                dropout=dropout,\n",
        "                activation='gelu',\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.LayerNorm([sequence_length, feature_dim]),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feature_dim * sequence_length, 256),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer(x)\n",
        "        return self.cls_head(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_video_paths, train_labels = get_video_paths_and_labels(\"/content/data_trimmed_restructured/Train\", CLASS_NAMES)\n",
        "test_video_paths, test_labels = get_video_paths_and_labels(\"/content/data_trimmed_restructured/Test\", CLASS_NAMES)\n",
        "\n",
        "\n",
        "#  class weights from training labels\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.arange(len(CLASS_NAMES)),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "#  datasets and loaders\n",
        "train_dataset = VideoDataset(train_video_paths, train_labels, sequence_length=16, img_size=(64, 64), class_names=CLASS_NAMES)\n",
        "test_dataset = VideoDataset(test_video_paths, test_labels, sequence_length=16, img_size=(64, 64), class_names=CLASS_NAMES)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "model = VideoTransformerClassifier(feature_dim=768, num_classes=len(CLASS_NAMES), sequence_length=16).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE93t09zPfh2",
        "outputId": "94d8b66b-6c49-4001-983b-4a140201abf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqNCTcnMZ8AC",
        "outputId": "5fcf2147-2850-4fae-9505-202f189ac3a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train Loss: 2.2027, Acc: 0.1882, AUC: 0.5165 | Test Loss: 1.9026, Acc: 0.2769, AUC: 0.5817\n",
            "Epoch 02 | Train Loss: 1.7736, Acc: 0.3833, AUC: 0.6301 | Test Loss: 1.7204, Acc: 0.3692, AUC: 0.5916\n",
            "Epoch 03 | Train Loss: 1.4296, Acc: 0.5122, AUC: 0.7061 | Test Loss: 1.7902, Acc: 0.4462, AUC: 0.6361\n",
            "Epoch 04 | Train Loss: 1.1401, Acc: 0.6237, AUC: 0.7790 | Test Loss: 1.8987, Acc: 0.3385, AUC: 0.6194\n",
            "Epoch 05 | Train Loss: 0.8365, Acc: 0.7073, AUC: 0.8321 | Test Loss: 1.9203, Acc: 0.3846, AUC: 0.6332\n",
            "Epoch 06 | Train Loss: 0.7150, Acc: 0.7631, AUC: 0.8540 | Test Loss: 2.1527, Acc: 0.4308, AUC: 0.6626\n",
            "Epoch 07 | Train Loss: 0.5306, Acc: 0.8258, AUC: 0.9071 | Test Loss: 2.3716, Acc: 0.3692, AUC: 0.6282\n",
            "Epoch 08 | Train Loss: 0.3981, Acc: 0.8920, AUC: 0.9395 | Test Loss: 2.5067, Acc: 0.3231, AUC: 0.6103\n",
            "Epoch 09 | Train Loss: 0.2558, Acc: 0.9268, AUC: 0.9599 | Test Loss: 2.8447, Acc: 0.3385, AUC: 0.6072\n",
            "Epoch 10 | Train Loss: 0.1596, Acc: 0.9652, AUC: 0.9803 | Test Loss: 2.6092, Acc: 0.4308, AUC: 0.6515\n"
          ]
        }
      ],
      "source": [
        "train_video_paths, train_labels = get_video_paths_and_labels(\"/content/data_trimmed_restructured/Train\", CLASS_NAMES)\n",
        "test_video_paths, test_labels = get_video_paths_and_labels(\"/content/data_trimmed_restructured/Test\", CLASS_NAMES)\n",
        "\n",
        "\n",
        "#  class weights from training labels\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.arange(len(CLASS_NAMES)),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "#  datasets and loaders\n",
        "train_dataset = VideoDataset(train_video_paths, train_labels, sequence_length=16, img_size=(64, 64), class_names=CLASS_NAMES)\n",
        "test_dataset = VideoDataset(test_video_paths, test_labels, sequence_length=16, img_size=(64, 64), class_names=CLASS_NAMES)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "model = VideoTransformerClassifier(feature_dim=768, num_classes=len(CLASS_NAMES), sequence_length=16).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "EPOCHS = 10\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    train_loss, train_preds, train_targets = 0, [], []\n",
        "\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "\n",
        "\n",
        "        # weighted cross-entropy\n",
        "        loss = F.cross_entropy(outputs, batch_y, weight=class_weights_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_loss += loss.item() * batch_x.size(0)\n",
        "        train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        train_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_targets, train_preds)\n",
        "    try:\n",
        "        train_auc = roc_auc_score(\n",
        "            F.one_hot(torch.tensor(train_targets), num_classes=len(CLASS_NAMES)),\n",
        "            F.one_hot(torch.tensor(train_preds), num_classes=len(CLASS_NAMES)),\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "    except:\n",
        "        train_auc = 0.0\n",
        "\n",
        "\n",
        "    # evaluate on test\n",
        "    model.eval()\n",
        "    test_loss, test_preds, test_targets = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            loss = F.cross_entropy(outputs, batch_y)\n",
        "\n",
        "\n",
        "            test_loss += loss.item() * batch_x.size(0)\n",
        "            test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            test_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = accuracy_score(test_targets, test_preds)\n",
        "    try:\n",
        "        test_auc = roc_auc_score(\n",
        "            F.one_hot(torch.tensor(test_targets), num_classes=len(CLASS_NAMES)),\n",
        "            F.one_hot(torch.tensor(test_preds), num_classes=len(CLASS_NAMES)),\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "    except:\n",
        "        test_auc = 0.0\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 5: Transformer Model Training & Fine-Tuning**\n",
        "(Create Transformer, train full model, apply class weights, data augmentation)"
      ],
      "metadata": {
        "id": "aMup-4KvFHrZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6laJLNYadPr"
      },
      "source": [
        "Fine Tuning Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "busMl-0Xaftv",
        "outputId": "af72c860-b5b7-4798-fb5a-b27363ac4143"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dinov2Model(\n",
              "  (embeddings): Dinov2Embeddings(\n",
              "    (patch_embeddings): Dinov2PatchEmbeddings(\n",
              "      (projection): Conv2d(3, 768, kernel_size=(14, 14), stride=(14, 14))\n",
              "    )\n",
              "    (dropout): Dropout(p=0.0, inplace=False)\n",
              "  )\n",
              "  (encoder): Dinov2Encoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-11): 12 x Dinov2Layer(\n",
              "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (attention): Dinov2Attention(\n",
              "          (attention): Dinov2SelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (output): Dinov2SelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (layer_scale1): Dinov2LayerScale()\n",
              "        (drop_path): Identity()\n",
              "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): Dinov2MLP(\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "        (layer_scale2): Dinov2LayerScale()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# fine tuning block 1 - Imports and DINOv2 Setup (same as before)\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "from PIL import Image, ImageEnhance\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import random\n",
        "\n",
        "\n",
        "# device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# DINOv2 setup\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "dinov2 = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(device)\n",
        "dinov2.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FGellvBajuG"
      },
      "outputs": [],
      "source": [
        "# fine-tuning block 2: dataset class with simple augmentations\n",
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, video_paths, labels, sequence_length, img_size, class_names, augment=False):\n",
        "        self.video_paths = video_paths\n",
        "        self.labels = labels\n",
        "        self.sequence_length = sequence_length\n",
        "        self.img_size = img_size\n",
        "        self.class_names = class_names\n",
        "        self.augment = augment\n",
        "\n",
        "\n",
        "    def augment_frame(self, image):\n",
        "        if random.random() < 0.5:\n",
        "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "        if random.random() < 0.3:\n",
        "            enhancer = ImageEnhance.Brightness(image)\n",
        "            image = enhancer.enhance(random.uniform(0.7, 1.3))\n",
        "        return image\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "\n",
        "        frame_paths = sorted([\n",
        "            os.path.join(video_path, f)\n",
        "            for f in os.listdir(video_path)\n",
        "            if f.endswith(\".png\")\n",
        "        ])\n",
        "\n",
        "\n",
        "        frames = []\n",
        "        for path in frame_paths[:self.sequence_length]:\n",
        "            img = Image.open(path).resize(self.img_size)\n",
        "            if self.augment:\n",
        "                img = self.augment_frame(img)\n",
        "            inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                features = dinov2(**inputs).last_hidden_state.mean(dim=1).squeeze(0)\n",
        "            frames.append(features)\n",
        "\n",
        "\n",
        "        while len(frames) < self.sequence_length:\n",
        "            frames.append(torch.zeros_like(frames[0]))\n",
        "\n",
        "\n",
        "        sequence = torch.stack(frames)\n",
        "        return sequence, torch.tensor(label, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rM7TuxeqapD9"
      },
      "outputs": [],
      "source": [
        "# fine tuning block 3: updated model with more dropout and simplified classifier\n",
        "class VideoTransformerClassifier(nn.Module):\n",
        "    def __init__(self, feature_dim, num_classes, sequence_length, dropout=0.4):\n",
        "        super().__init__()\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, sequence_length, feature_dim))\n",
        "\n",
        "\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=feature_dim,\n",
        "                nhead=4,\n",
        "                dim_feedforward=512,\n",
        "                dropout=dropout,\n",
        "                activation='gelu',\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=2\n",
        "        )\n",
        "\n",
        "\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.LayerNorm([sequence_length, feature_dim]),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feature_dim * sequence_length, 128),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer(x)\n",
        "        return self.cls_head(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7ZfQ2uOar8i"
      },
      "outputs": [],
      "source": [
        "# fine tuning block 4: class weights + sampler\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "train_video_paths, train_labels = get_video_paths_and_labels(\"/content/data_trimmed_restructured/Train\", CLASS_NAMES)\n",
        "test_video_paths, test_labels = get_video_paths_and_labels(\"/content/data_trimmed_restructured/Test\", CLASS_NAMES)\n",
        "\n",
        "\n",
        "# class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.arange(len(CLASS_NAMES)), y=train_labels)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "\n",
        "# Sampler to balance classes in training\n",
        "class_counts = Counter(train_labels)\n",
        "sample_weights = [1.0 / class_counts[label] for label in train_labels]\n",
        "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
        "\n",
        "\n",
        "#  sampler + augmentation for train only\n",
        "train_dataset = VideoDataset(train_video_paths, train_labels, sequence_length=16, img_size=(64, 64), class_names=CLASS_NAMES, augment=True)\n",
        "test_dataset = VideoDataset(test_video_paths, test_labels, sequence_length=16, img_size=(64, 64), class_names=CLASS_NAMES, augment=False)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpBTHt7faw5O",
        "outputId": "4150cffa-9bd0-42a3-ee89-413bf8a8c221"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 01 | Train Loss: 2.0715, Acc: 0.1777, AUC: 0.5317 | Test Loss: 2.0612, Acc: 0.1846, AUC: 0.5341\n",
            "Epoch 02 | Train Loss: 1.7181, Acc: 0.3380, AUC: 0.6138 | Test Loss: 1.9660, Acc: 0.2769, AUC: 0.5690\n",
            "Epoch 03 | Train Loss: 1.4954, Acc: 0.4286, AUC: 0.6684 | Test Loss: 2.0480, Acc: 0.2923, AUC: 0.6021\n",
            "Epoch 04 | Train Loss: 1.2458, Acc: 0.4599, AUC: 0.6913 | Test Loss: 1.9901, Acc: 0.2615, AUC: 0.5746\n",
            "Epoch 05 | Train Loss: 0.9589, Acc: 0.6202, AUC: 0.7911 | Test Loss: 2.1338, Acc: 0.3077, AUC: 0.6080\n",
            "Epoch 06 | Train Loss: 0.8999, Acc: 0.6237, AUC: 0.7837 | Test Loss: 2.0965, Acc: 0.2769, AUC: 0.5619\n",
            "Epoch 07 | Train Loss: 0.6995, Acc: 0.6899, AUC: 0.8130 | Test Loss: 2.0629, Acc: 0.2615, AUC: 0.5620\n",
            "Epoch 08 | Train Loss: 0.4842, Acc: 0.8153, AUC: 0.8864 | Test Loss: 2.4287, Acc: 0.2923, AUC: 0.5926\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fine tuning block  5: training loop with early stopping + metrics\n",
        "model = VideoTransformerClassifier(feature_dim=768, num_classes=len(CLASS_NAMES), sequence_length=16).to(device)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "best_test_auc = 0.0\n",
        "patience, patience_counter = 3, 0  # early stopping\n",
        "\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    model.train()\n",
        "    train_loss, train_preds, train_targets = 0, [], []\n",
        "\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = F.cross_entropy(outputs, batch_y, weight=class_weights_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_loss += loss.item() * batch_x.size(0)\n",
        "        train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        train_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_targets, train_preds)\n",
        "    train_auc = roc_auc_score(\n",
        "        F.one_hot(torch.tensor(train_targets), num_classes=len(CLASS_NAMES)),\n",
        "        F.one_hot(torch.tensor(train_preds), num_classes=len(CLASS_NAMES)),\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    test_loss, test_preds, test_targets = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            loss = F.cross_entropy(outputs, batch_y)\n",
        "            test_loss += loss.item() * batch_x.size(0)\n",
        "            test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            test_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = accuracy_score(test_targets, test_preds)\n",
        "    test_auc = roc_auc_score(\n",
        "        F.one_hot(torch.tensor(test_targets), num_classes=len(CLASS_NAMES)),\n",
        "        F.one_hot(torch.tensor(test_preds), num_classes=len(CLASS_NAMES)),\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f} | Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n",
        "\n",
        "\n",
        "    # early stopping\n",
        "    if test_auc > best_test_auc:\n",
        "        best_test_auc = test_auc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model.pt\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "#  best saved model & freeze base layers\n",
        "model = VideoTransformerClassifier(feature_dim=768, num_classes=len(CLASS_NAMES), sequence_length=16).to(device)\n",
        "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 6: Further Fine-Tuning**\n",
        "(Freeze DINOv2 backbone, train only classifier head)"
      ],
      "metadata": {
        "id": "Oz9vd7ToFZlX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJEeMSZma3Jr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "JdOXiqfia5b2",
        "outputId": "636d1ab8-7f75-4c54-cf9f-94bba545bc89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[FT-Classifier] Epoch 01 | Train Loss: 0.8458, Acc: 0.6516, AUC: 0.8092 | Test Loss: 2.1463, Acc: 0.2769, AUC: 0.5777\n",
            "[FT-Classifier] Epoch 02 | Train Loss: 0.7266, Acc: 0.6829, AUC: 0.8149 | Test Loss: 2.2951, Acc: 0.2769, AUC: 0.5769\n",
            "[FT-Classifier] Epoch 03 | Train Loss: 0.6589, Acc: 0.7143, AUC: 0.8339 | Test Loss: 2.3767, Acc: 0.2615, AUC: 0.5697\n",
            "[FT-Classifier] Epoch 04 | Train Loss: 0.5705, Acc: 0.7561, AUC: 0.8535 | Test Loss: 2.1546, Acc: 0.3231, AUC: 0.5974\n",
            "[FT-Classifier] Epoch 05 | Train Loss: 0.5893, Acc: 0.7596, AUC: 0.8671 | Test Loss: 2.3098, Acc: 0.3077, AUC: 0.5991\n",
            "[FT-Classifier] Epoch 06 | Train Loss: 0.4679, Acc: 0.8188, AUC: 0.8818 | Test Loss: 2.2899, Acc: 0.3231, AUC: 0.6055\n",
            "[FT-Classifier] Epoch 07 | Train Loss: 0.4332, Acc: 0.8258, AUC: 0.9014 | Test Loss: 2.3155, Acc: 0.2923, AUC: 0.5830\n",
            "[FT-Classifier] Epoch 08 | Train Loss: 0.3528, Acc: 0.8571, AUC: 0.9200 | Test Loss: 2.3911, Acc: 0.3077, AUC: 0.5898\n",
            "[FT-Classifier] Epoch 09 | Train Loss: 0.3959, Acc: 0.8537, AUC: 0.9112 | Test Loss: 2.3991, Acc: 0.3077, AUC: 0.5938\n",
            "Early stopping triggered on classifier fine-tune.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg4AAAHWCAYAAAAfPgHmAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAApnZJREFUeJzs3XdYFNfXB/Dv0pEqKIqIgNIExYYaxQL2LjYsqGCNXWPH3mKL3URji2KJJfbeWyyJiGJFYhcERVGaIm3P+wcv82MFdGk7s3o+eeaJ3J1y9s6Ws7fMyIiIwBhjjDGmBA2xA2CMMcaY+uDEgTHGGGNK48SBMcYYY0rjxIExxhhjSuPEgTHGGGNK48SBMcYYY0rjxIExxhhjSuPEgTHGGGNK48SBMcYYY0rjxIGxfHj48CGaNWsGExMTyGQy7N+/v1D3/+zZM8hkMmzatKlQ96vOPD094enpKXYYLAebNm2CTCbDs2fP8rztjBkzIJPJCj8oVmQ4cWBq6/Hjx/jxxx9Rvnx56OnpwdjYGB4eHli+fDmSkpKK9Nh+fn64c+cOfv75Z2zZsgXu7u5FejxV8vf3h0wmg7GxcY71+PDhQ8hkMshkMixatCjP+4+MjMSMGTMQEhJSCNGqhq2trfCcZTIZ9PT04ODggHHjxuHdu3dFdtyjR49ixowZSq/v6ekJmUwGBweHHB8/deqU8Bx2795dSFGy742W2AEwlh9HjhxBly5doKuri969e6NSpUpISUnBpUuXMG7cONy7dw9r164tkmMnJSXh6tWrmDx5MoYNG1Ykx7CxsUFSUhK0tbWLZP9fo6WlhY8fP+LQoUPw8fFReGzbtm3Q09PDp0+f8rXvyMhIzJw5E7a2tqhatarS2508eTJfxyssVatWxZgxYwAAnz59QnBwMJYtW4YLFy7g2rVrRXLMo0eP4rfffstT8qCnp4dHjx7h2rVrqFWrlsJjBT13jAGcODA19PTpU3Tr1g02NjY4e/YsLC0thceGDh2KR48e4ciRI0V2/Ddv3gAATE1Ni+wYmb9qxaKrqwsPDw9s3749W+Lw559/onXr1tizZ49KYvn48SOKFSsGHR0dlRwvN1ZWVujZs6fwd//+/WFoaIhFixbh4cOHuf7KV7UKFSogLS0N27dvV0gcPn36hH379qn03LFvE3dVMLWzcOFCJCYmYsOGDQpJQyZ7e3uMHDlS+DstLQ2zZ89GhQoVoKurC1tbW0yaNAnJyckK29na2qJNmza4dOkSatWqBT09PZQvXx6bN28W1pkxYwZsbGwAAOPGjYNMJoOtrS2AjCb+zH9nlVMf7qlTp1CvXj2YmprC0NAQTk5OmDRpkvB4bmMczp49i/r168PAwACmpqZo3749QkNDczzeo0eP4O/vD1NTU5iYmKBPnz74+PFj7hX7mR49euDYsWOIjY0VyoKCgvDw4UP06NEj2/rv3r3D2LFjUblyZRgaGsLY2BgtW7bErVu3hHXOnz+PmjVrAgD69OkjNJtnPk9PT09UqlQJwcHBaNCgAYoVKybUy+djHPz8/KCnp5ft+Tdv3hzFixdHZGSk0s81v0qXLg0go4UmqwcPHqBz584wMzODnp4e3N3dcfDgQYV1UlNTMXPmTDg4OEBPTw/m5uaoV68eTp06BSDj9fTbb78BgEI3iTK6d++OnTt3Qi6XC2WHDh3Cx48fsyWCmW7evImWLVvC2NgYhoaGaNy4Mf75559s6927dw+NGjWCvr4+ypYtizlz5igcJ6tjx44Jr1cjIyO0bt0a9+7dU+o5MOniFgemdg4dOoTy5cujbt26Sq3fv39/BAYGonPnzhgzZgz+/fdfzJs3D6Ghodi3b5/Cuo8ePULnzp3Rr18/+Pn54Y8//oC/vz9q1KgBV1dXdOzYEaampvjpp5/QvXt3tGrVCoaGhnmK/969e2jTpg3c3Nwwa9Ys6Orq4tGjR7h8+fIXtzt9+jRatmyJ8uXLY8aMGUhKSsLKlSvh4eGBGzduZEtafHx8YGdnh3nz5uHGjRtYv349LCwssGDBAqXi7NixIwYNGoS9e/eib9++ADJaG5ydnVG9evVs6z958gT79+9Hly5dYGdnh9evX2PNmjVo2LAh7t+/jzJlyqBixYqYNWsWpk2bhoEDB6J+/foAoHAuY2Ji0LJlS3Tr1g09e/ZEqVKlcoxv+fLlOHv2LPz8/HD16lVoampizZo1OHnyJLZs2YIyZcoo9TyVlZqairdv3wLI+PV+8+ZNLFmyBA0aNICdnZ2w3r179+Dh4QErKytMnDgRBgYG2LVrF7y9vbFnzx506NABQEaCN2/ePPTv3x+1atVCfHw8rl+/jhs3bqBp06b48ccfERkZiVOnTmHLli15irVHjx6YMWMGzp8/j0aNGgHIOHeNGzeGhYVFtvXv3buH+vXrw9jYGOPHj4e2tjbWrFkDT09PXLhwAbVr1wYAvHr1Cl5eXkhLSxOe29q1a6Gvr59tn1u2bIGfnx+aN2+OBQsW4OPHj1i9ejXq1auHmzdv5phkMzVBjKmRuLg4AkDt27dXav2QkBACQP3791coHzt2LAGgs2fPCmU2NjYEgC5evCiURUdHk66uLo0ZM0Yoe/r0KQGgX375RWGffn5+ZGNjky2G6dOnU9a32tKlSwkAvXnzJte4M4+xceNGoaxq1apkYWFBMTExQtmtW7dIQ0ODevfune14ffv2Vdhnhw4dyNzcPNdjZn0eBgYGRETUuXNnaty4MRERpaenU+nSpWnmzJk51sGnT58oPT092/PQ1dWlWbNmCWVBQUHZnlumhg0bEgD6/fffc3ysYcOGCmUnTpwgADRnzhx68uQJGRoakre391efY15lvjY+Xzw8POjt27cK6zZu3JgqV65Mnz59EsrkcjnVrVuXHBwchLIqVapQ69atv3jcoUOHUl4+phs2bEiurq5EROTu7k79+vUjIqL379+Tjo4OBQYG0rlz5wgA/fXXX8J23t7epKOjQ48fPxbKIiMjycjIiBo0aCCUjRo1igDQv//+K5RFR0eTiYkJAaCnT58SEVFCQgKZmprSgAEDFOJ79eoVmZiYKJR//v5g0sddFUytxMfHAwCMjIyUWv/o0aMAgNGjRyuUZw5y+3wshIuLi/ArGABKliwJJycnPHnyJN8xfy5zbMSBAwdybeL9XFRUFEJCQuDv7w8zMzOh3M3NDU2bNhWeZ1aDBg1S+Lt+/fqIiYkR6lAZPXr0wPnz5/Hq1SucPXsWr169yrGbAsgYF6GhkfGRkp6ejpiYGKEb5saNG0ofU1dXF3369FFq3WbNmuHHH3/ErFmz0LFjR+jp6WHNmjVKHysvateujVOnTuHUqVM4fPgwfv75Z9y7dw/t2rUTZp+8e/cOZ8+ehY+PDxISEvD27Vu8ffsWMTExaN68OR4+fIiXL18CyHgd3Lt3Dw8fPiySeHv06IG9e/ciJSUFu3fvhqamptDakVV6ejpOnjwJb29vlC9fXii3tLREjx49cOnSJeE1c/ToUfzwww8KYydKliwJX19fhX2eOnUKsbGx6N69u1AHb9++haamJmrXro1z584VyXNmqsGJA1MrxsbGAICEhASl1n/+/Dk0NDRgb2+vUF66dGmYmpri+fPnCuXlypXLto/ixYvj/fv3+Yw4u65du8LDwwP9+/dHqVKl0K1bN+zateuLSURmnE5OTtkeq1ixIt6+fYsPHz4olH/+XIoXLw4AeXourVq1gpGREXbu3Ilt27ahZs2a2eoyk1wux9KlS+Hg4ABdXV2UKFECJUuWxO3btxEXF6f0Ma2srPI0EHLRokUwMzNDSEgIVqxYkWNT/OfevHmDV69eCUtiYuJXtylRogSaNGmCJk2aoHXr1pg0aRLWr1+PK1euYP369QAyurqICFOnTkXJkiUVlunTpwMAoqOjAQCzZs1CbGwsHB0dUblyZYwbNw63b99W+nl/Tbdu3RAXF4djx45h27ZtaNOmTY4J95s3b/Dx48dcX1tyuRzh4eEAMl6HOQ0C/XzbzGSoUaNG2erh5MmTQh0w9cRjHJhaMTY2RpkyZXD37t08bafsoDJNTc0cy4ko38dIT09X+FtfXx8XL17EuXPncOTIERw/fhw7d+5Eo0aNcPLkyVxjyKuCPJdMurq66NixIwIDA/HkyZMvTgucO3cupk6dir59+2L27NkwMzODhoYGRo0apXTLCoAc+8u/5ObNm8IX0Z07d9C9e/evblOzZk2FpHH69Ol5mvKYqXHjxgCAixcvYvjw4cLzHDt2LJo3b57jNpmJV4MGDfD48WMcOHAAJ0+exPr167F06VL8/vvv6N+/f55j+ZylpSU8PT2xePFiXL58WaUzKTLrYcuWLcIA0qw+H0zK1AufPaZ22rRpg7Vr1+Lq1auoU6fOF9e1sbGBXC7Hw4cPUbFiRaH89evXiI2NFWZIFIbixYsrzEDI9HmrBgBoaGigcePGaNy4MZYsWYK5c+di8uTJOHfuHJo0aZLj8wCAsLCwbI89ePAAJUqUgIGBQcGfRA569OiBP/74AxoaGujWrVuu6+3evRteXl7YsGGDQnlsbCxKlCgh/F2YVwn88OED+vTpAxcXF9StWxcLFy5Ehw4dhJkbudm2bZvCxa2yNtHnRVpaGgAILRaZ+9HW1s7xPH7OzMwMffr0QZ8+fZCYmIgGDRpgxowZQuJQ0Lrq0aMH+vfvD1NTU7Rq1SrHdUqWLIlixYrl+trS0NCAtbU1gIzXYU5dK59vW6FCBQCAhYWFUvXA1At3VTC1M378eBgYGKB///54/fp1tscfP36M5cuXA4DwYbls2TKFdZYsWQIAaN26daHFVaFCBcTFxSk0N0dFRWWbuZHTlQYzL4T0+RTRTJaWlqhatSoCAwMVkpO7d+/i5MmTuX4pFAYvLy/Mnj0bv/76a46/HjNpampma83466+/hD79TJkJTk5JVl5NmDABL168QGBgIJYsWQJbW1v4+fnlWo+ZPDw8hG6HJk2a5DtxOHToEACgSpUqADK+KD09PbFmzRpERUVlWz/zGiBAxuyRrAwNDWFvb68Qe0HrqnPnzpg+fTpWrVqVa/ePpqYmmjVrhgMHDihcMvr169f4888/Ua9ePaGLsFWrVvjnn38ULnj15s0bbNu2TWGfzZs3h7GxMebOnYvU1NRsx8xaD0z9cIsDUzsVKlTAn3/+ia5du6JixYoKV468cuUK/vrrL/j7+wPI+ED38/PD2rVrERsbi4YNG+LatWsIDAyEt7c3vLy8Ci2ubt26YcKECejQoQNGjBghTD9zdHRUGBw4a9YsXLx4Ea1bt4aNjQ2io6OxatUqlC1bFvXq1ct1/7/88gtatmyJOnXqoF+/fsJ0TBMTk3w1sytLQ0MDU6ZM+ep6bdq0waxZs9CnTx/UrVsXd+7cwbZt27J9KVeoUAGmpqb4/fffYWRkBAMDA9SuXVthSqMyzp49i1WrVmH69OnC9NCNGzfC09MTU6dOxcKFC/O0v695+fIltm7dCgBISUnBrVu3sGbNGpQoUQLDhw8X1vvtt99Qr149VK5cGQMGDED58uXx+vVrXL16FREREcJ1LVxcXODp6YkaNWrAzMwM169fx+7duxWuRlqjRg0AwIgRI9C8eXNoamp+sdXnc8q+NubMmSNcW2TIkCHQ0tLCmjVrkJycrFCP48ePx5YtW9CiRQuMHDlSmI5pY2OjkDAbGxtj9erV6NWrF6pXr45u3bqhZMmSePHiBY4cOQIPDw/8+uuvSj8PJjHiTupgLP/+++8/GjBgANna2pKOjg4ZGRmRh4cHrVy5UmEqXGpqKs2cOZPs7OxIW1ubrK2tKSAgQGEdoowpdzlNj/t8GmBu0zGJiE6ePEmVKlUiHR0dcnJyoq1bt2abbnbmzBlq3749lSlThnR0dKhMmTLUvXt3+u+//7Id4/Mpi6dPnyYPDw/S19cnY2Njatu2Ld2/f19hnczjfT7dc+PGjQpT5nKTdTpmbnKbjjlmzBiytLQkfX198vDwoKtXr+Y4jfLAgQPk4uJCWlpaCs8z63TCz2XdT3x8PNnY2FD16tUpNTVVYb2ffvqJNDQ06OrVq198Dnnx+XRMDQ0NsrCwoO7du9OjR4+yrf/48WPq3bs3lS5dmrS1tcnKyoratGlDu3fvFtaZM2cO1apVi0xNTUlfX5+cnZ3p559/ppSUFGGdtLQ0Gj58OJUsWZJkMtlXpy1+qf4y5TQdk4joxo0b1Lx5czI0NKRixYqRl5cXXblyJdv2t2/fpoYNG5Kenh5ZWVnR7NmzacOGDTm+ts6dO0fNmzcnExMT0tPTowoVKpC/vz9dv35dWIenY6ofGVEeRkoxxhhj7LvGYxwYY4wxpjROHBhjjDGmNE4cGGOMMaY0ThwYY4yx78TLly/Rs2dPmJubQ19fH5UrV8b169fztA+ejskYY4x9B96/fw8PDw94eXnh2LFjKFmyJB4+fChcjl5ZPKuCMcYY+w5MnDgRly9fxt9//12g/XDiwHIll8sRGRkJIyOjQr1MMGOMSR0RISEhAWXKlBHu+lpUPn36hJSUlHxvT0TZPqN1dXWhq6urUObi4oLmzZsjIiICFy5cgJWVFYYMGYIBAwbk+YCM5Sg8PFzhoje88MILL9/bEh4eXqSfs0lJSQStYgWK0dDQMFvZ9OnTsx1LV1eXdHV1KSAggG7cuEFr1qwhPT092rRpU55i5hYHlqu4uLiMSwMfvw59A0Oxw8lRCxdLsUNQa8fvZ7+fgpTw+S24528+fH0lEdmULJqbsxVUQnw87O2sERsbCxMTkyI7Tnx8PExMTKDr4gdoKn87eUF6CpLvByI8PFy4pwiQc4uDjo4O3N3dceXKFaFsxIgRCAoKwtWrV5U+JA+OZLnKbPrSNzBEMUMjkaPJWdY3Csu7YoaJYofwRXx+C87wU+Hcpr2oGBtLM3HIpLJuWi09yPKROJAsoxvF2Nj4q+8XS0tLuLi4KJRVrFgxz7dc58SBMcYYE5sMQH6SlDxs4uHhke0W6P/99x9sbGzydEhOHBhjjDGxyTQylvxsp6SffvoJdevWxdy5c+Hj44Nr165h7dq1WLt2bZ4OyReAYowxxsQmk+V/UVLNmjWxb98+bN++HZUqVcLs2bOxbNky+Pr65ilUbnFgjDHGvhNt2rRBmzZtCrQPThwYY4wxsamgq6KwcOLAGGOMiS2P3Q4K26kYJw6MMcaY6PLZ4iDCUEVOHBhjjDGxqVGLA8+qYIwxxpjSuMWBMcYYExsPjmSMMcaY0tSoq4ITB1akDh6+guvBYYh6FQNtbS042JdFty5esLQ0Fzs0Bet2XcDKrWcQHROPSg5WWDCuC2q42oodlkDK8anDOZZy/QHSji/47hNs3nMRoY8i8PZdAhZP6Q2vOq5ih5WNlOtQKWrU4sBjHFiRehD2Ak0a18D0KX6YMLY70tPTsWDxdnxKzv+95wvb3pPBmLJsHyb0b4nzWyagkoMVOg3/DW/eJYgdGgDpxyf1cyz1+pN6fJ8+pcDRzhITB3uLHUqupF6HSlHBlSMLCycORezq1avQ1NRE69atRYvB398f3t7eohx7/JhuaFDPDWWtSsKmXCkM7NcGMTHxePbslSjx5GTVn2fR27sufNvVgXN5SywJ6IZiejrYelD528wWJanHJ/VzLPX6k3p8Hu7OGNq7ORrVrSR2KLmSeh1+azhxKGIbNmzA8OHDcfHiRURGRua6HhEhLS0tW3lKijR+tRWWpKRkAICBgZ7IkWRISU1DyINweNZyEso0NDTQsJYTgu48FTGyDFKPLydSOsdSrz+px6cOvpk6zOyqyM+iYpw4FKHExETs3LkTgwcPRuvWrbFp0ybhsfPnz0Mmk+HYsWOoUaMGdHV1cenSJXh6emLYsGEYNWoUSpQogebNmwMA7t69i5YtW8LQ0BClSpVCr1698PbtW2F/u3fvRuXKlaGvrw9zc3M0adIEHz58wIwZMxAYGIgDBw5AJpNBJpPh/PnzKq6JDHI5Yev203B0KAvrshaixPC5mNhEpKfLUdLMSKG8pJkxomPiRYrqf6Qe3+ekdo6lXn9Sj08dfDN1KJPlM3Hgropvyq5du+Ds7AwnJyf07NkTf/zxB4hIYZ2JEydi/vz5CA0NhZubGwAgMDAQOjo6uHz5Mn7//XfExsaiUaNGqFatGq5fv47jx4/j9evX8PHxAQBERUWhe/fu6Nu3L0JDQ3H+/Hl07NgRRISxY8fCx8cHLVq0QFRUFKKiolC3bt0c401OTkZ8fLzCUpgCtx5HRMQbDB3kXaj7ZdLB55ixfNKQ5X9RMZ5VUYQ2bNiAnj17AgBatGiBuLg4XLhwAZ6ensI6s2bNQtOmTRW2c3BwwMKFC4W/58yZg2rVqmHu3LlC2R9//AFra2v8999/SExMRFpaGjp27AgbGxsAQOXKlYV19fX1kZycjNKlS38x3nnz5mHmzJn5fr5fErjlBEJCHmFyQC+YmRkXyTHyw9zUEJqaGtkGUb15Fw8Lc/HjlHp8WUnxHEu9/qQenzr4ZuqQZ1WwsLAwXLt2Dd27dwcAaGlpoWvXrtiwYYPCeu7u7tm2rVGjhsLft27dwrlz52BoaCgszs7OAIDHjx+jSpUqaNy4MSpXrowuXbpg3bp1eP/+fZ5jDggIQFxcnLCEh4fneR+fIyIEbjmB4BthCBjvC4uSpgXeZ2HS0dZCVWdrXAgKE8rkcjkuBv2HmpXtRIwsg9TjA6R9jqVef1KPTx1wHaoetzgUkQ0bNiAtLQ1lypQRyogIurq6+PXXX4UyAwODbNt+XpaYmIi2bdtiwYIF2da1tLSEpqYmTp06hStXruDkyZNYuXIlJk+ejH///Rd2dsq/cXR1daGrq6v0+soI3HICV/+5h1EjOkNPXwexcYkAgGL6utDR0S7UY+XXkB6NMGTmFlSrWA7VXW2xevs5fEhKhm/bH8QODYD045P6OZZ6/Uk9vo9JyQiPjBH+fvnqHcIeR8LYSB+WFsVFjOx/pF6HSuELQH3f0tLSsHnzZixevBjNmjVTeMzb2xvbt28XWgyUUb16dezZswe2trbQ0sr5lMlkMnh4eMDDwwPTpk2DjY0N9u3bh9GjR0NHRwfp6ekFek75debcDQDA3AXbFMoH9GuDBvXcxAgpm47NauBtbCLmrjmC6JgEVHa0wu4VQyXTzCn1+KR+jqVef1KP7/7DCAwMWCv8vWT9YQBA28Y1MHO0j1hhKZB6HSpFjboqZPT5aD1WYPv370fXrl0RHR0NExMThccmTJiAs2fP4pdffoGXlxfev38PU1NT4XFPT09UrVoVy5YtE8oiIyNRtWpVNGzYEOPHj4eZmRkePXqEHTt2YP369bh+/TrOnDmDZs2awcLCAv/++y969uyJ/fv3o2XLlpg7dy7WrFmDkydPwtzcHCYmJtDW/vovwfj4eJiYmCDw7wcoZmj01fXF0KZSma+vxHJ1+G7uU4SlgM9vwT2N/iB2CF9kZ5G91VUK4uPjUcrcBHFxcTA2LroEJPNzVtdzBmRaeZ/CTGmfkHx+RpHHmRWPcSgCGzZsQJMmTbIlDQDQqVMnXL9+Hbdv31Z6f2XKlMHly5eRnp6OZs2aoXLlyhg1ahRMTU2hoaEBY2NjXLx4Ea1atYKjoyOmTJmCxYsXo2XLlgCAAQMGwMnJCe7u7ihZsiQuX75caM+VMcZYIVCj6zhwiwPLFbc4fPu4xeHbxy0O+aPyFgevWflvcTg3TaUtDjzGgTHGGBMbD45kjDHGmNLUaHAkJw6MMcaY2LjFgTHGGGPKy+9AR75yJGOMMcYkjFscGGOMMbFxVwVjjDHGlJZ5W+38bKdinDgwxhhjYuNZFYwxxhhTmhp1VfDgSMYYY4wpjVscGGOMMbFxVwVjjDHGlKZGXRWcODDGGGNi4xYH9i1p4WKpsruu5ZXU7+547kms2CF80bAfbMQOQe3x3ScLRqrv4Y+JCao9oBq1OPDgSMYYY4wpjVscGGOMMZHJZDLI1KTFgRMHxhhjTGScODDGGGNMebL/X/KznYpx4sAYY4yJTJ1aHHhwJGOMMcaUxi0OjDHGmMjUqcWBEwfGGGNMZOqUOHBXBWOMMSayzMQhP4uyZsyYkW1bZ2fnPMfKLQ6MMcaY2FQ0q8LV1RWnT58W/tbSynsawIkDY4wx9p3Q0tJC6dKlC7QP7qpgjDHGRKaKrgoAePjwIcqUKYPy5cvD19cXL168yHOs3OLAGGOMiSzjHlf5GRyZ8b/4+HiFYl1dXejq6iqU1a5dG5s2bYKTkxOioqIwc+ZM1K9fH3fv3oWRkZHSh+TEganEul0XsHLrGUTHxKOSgxUWjOuCGq62YoeFg4ev4HpwGKJexUBbWwsO9mXRrYsXLC3NxQ5NUMe2OOraFoeZvjYA4FVCMk799xYPohNFjixD8N0n2LznIkIfReDtuwQsntIbXnVcxQ5LgVRffwDXX0Gpw3tYGTLkc1bF/2cO1tbWCqXTp0/HjBkzFMpatmwp/NvNzQ21a9eGjY0Ndu3ahX79+il9RO6qYEVu78lgTFm2DxP6t8T5LRNQycEKnYb/hjfvVHzb2hw8CHuBJo1rYPoUP0wY2x3p6elYsHg7PiWniB2aIC4pFUfuR2PpxadYevEpHr39gD61rFHKSPfrG6vAp08pcLSzxMTB3mKHkiMpv/4Arr+CUof3sDIK2lURHh6OuLg4YQkICPjqMU1NTeHo6IhHjx7lKVZOHFTk6tWr0NTUROvWrcUOReVW/XkWvb3rwrddHTiXt8SSgG4opqeDrQevih0axo/phgb13FDWqiRsypXCwH5tEBMTj2fPXokdmuD+60Q8iE7E2w8pePshBccevEFKmhw2xfXFDg0A4OHujKG9m6NR3Upih5IjKb/+AK6/glKH97AqGBsbKyyfd1PkJDExEY8fP4alpWWejsWJg4ps2LABw4cPx8WLFxEZGZnrekSEtLQ0FUZWtFJS0xDyIByetZyEMg0NDTSs5YSgO09FjCxnSUnJAAADAz2RI8mZDEDVMsbQ0ZTh+buPYocjeer2+pMadaw/qb+HcyUrwKKksWPH4sKFC3j27BmuXLmCDh06QFNTE927d89TqJw4qEBiYiJ27tyJwYMHo3Xr1ti0aZPw2Pnz5yGTyXDs2DHUqFEDurq6uHTpEm7dugUvLy8YGRnB2NgYNWrUwPXr14Xt9uzZA1dXV+jq6sLW1haLFy9WOKatrS3mzp2Lvn37wsjICOXKlcPatWtV9ZQFMbGJSE+Xo6SZ4sCbkmbGiI6Jz2UrccjlhK3bT8PRoSysy1qIHY6C0ka6mNvKGQvaVETnKpbYGBSB14nq1RQrBnV6/UmRutWflN/DX5Xfboo8jIuIiIhA9+7d4eTkBB8fH5ibm+Off/5ByZIl8xQqJw4qsGvXLjg7O8PJyQk9e/bEH3/8ASJSWGfixImYP38+QkND4ebmBl9fX5QtWxZBQUEIDg7GxIkToa2dMTguODgYPj4+6NatG+7cuYMZM2Zg6tSpCgkJACxevBju7u64efMmhgwZgsGDByMsLCzXOJOTkxEfH6+wfE8Ctx5HRMQbDB3kLXYo2bxJTMbiC4+x4u+nuPLsPbpXK4NShjpih8WYpEj5Pfw1qpiOuWPHDkRGRiI5ORkRERHYsWMHKlSokOdYOXFQgQ0bNqBnz54AgBYtWiAuLg4XLlxQWGfWrFlo2rQpKlSoADMzM7x48QJNmjSBs7MzHBwc0KVLF1SpUgUAsGTJEjRu3BhTp06Fo6Mj/P39MWzYMPzyyy8K+2zVqhWGDBkCe3t7TJgwASVKlMC5c+dyjXPevHkwMTERls9H6eaHuakhNDU1sg2kevMuHhbmxgXef2EJ3HICISGPEDDBF2Zm0okrUzoBMR9SERH3CUdDoxEZ/wn1y6vXqHExqMvrT6rUqf6k/h7+GlVdx6EwcOJQxMLCwnDt2jWhD0lLSwtdu3bFhg0bFNZzd3dX+Hv06NHo378/mjRpgvnz5+Px48fCY6GhofDw8FBY38PDAw8fPkR6erpQ5ubmJvxbJpOhdOnSiI6OzjXWgIAAhVG54eHheX/Cn9HR1kJVZ2tcCPpfS4dcLsfFoP9Qs7JdgfdfUESEwC0nEHwjDAHjfWFR0lTskJQigwxaGqr/wFA3Un/9SZ061J+6vofVGV/HoYht2LABaWlpKFOmjFBGRNDV1cWvv/4qlBkYGChsN2PGDPTo0QNHjhzBsWPHMH36dOzYsQMdOnRQ+tiZXRuZZDIZ5HJ5ruvndMGQwjCkRyMMmbkF1SqWQ3VXW6zefg4fkpLh2/aHQj9WXgVuOYGr/9zDqBGdoaevg9i4jGsjFNPXhY6O9le2Vo1WFS3w4HUi3ielQldLA9XLmqBCiWJY90/er/hWFD4mJSM8Mkb4++Wrdwh7HAljI31YWhQXMbIMUn79AVx/BaUO72GlqOheFYWBE4cilJaWhs2bN2Px4sVo1qyZwmPe3t7Yvn37F+9M5ujoCEdHR/z000/o3r07Nm7ciA4dOqBixYq4fPmywrqXL1+Go6MjNDU1i+S5FETHZjXwNjYRc9ccQXRMAio7WmH3iqGSaOo8c+4GAGDugm0K5QP6tUGDem45baJyhjqa6F69DIx1tZCUJkdU/Ces++cF/nvzQezQAAD3H0ZgYMD/Bt4uWX8YANC2cQ3MHO0jVlgCKb/+AK6/glKH97Ay8tvtIEZXBScORejw4cN4//49+vXrBxMTE4XHOnXqhA0bNmQblwAASUlJGDduHDp37gw7OztEREQgKCgInTp1AgCMGTMGNWvWxOzZs9G1a1dcvXoVv/76K1atWqWS55UfA30aYqBPQ7HDyGbLxklih/BVu25FiR3CF7m7VcCNIwvEDuOLpPr6A7j+Ckod3sPKUKfEgcc4FKENGzagSZMm2ZIGICNxuH79Om7fvp3tMU1NTcTExKB3795wdHSEj48PWrZsiZkzZwIAqlevjl27dmHHjh2oVKkSpk2bhlmzZsHf37+onxJjjLEioE6DI7nFoQgdOnQo18dq1aolTMkcMWKEwmM6OjrYvn37F/fdqVMnoQUiJ8+ePctWFhIS8sV9MsYYY1/DiQNjjDEmMnXqquDEgTHGGBMbz6pgjDHGmLK4xYExxhhjSlOnxIFnVTDGGGNMadziwBhjjIlMnVocOHFgjDHGxMaDIxljjDGmLG5xYIwxxpjS1Clx4MGRjDHGGFMatzgwxhhjIpMhny0OIgxy4MSBMcYYE5k6dVVw4sAYY4yJjWdVMKYabSqVETuEL5J6fE+jP4gdgtq7Fx0ndghfZGdhIHYIXyTV90h8fLxKj6dOLQ48OJIxxhhjSuMWB8YYY0xk6tTiwIkDY4wxJjKZLGPJz3aqxokDY4wxJrKMxCE/LQ5FEMxXcOLAGGOMiS2fLQ5izKrgwZGMMcYYUxq3ODDGGGMi48GRjDHGGFMaD45kjDHGmNI0NGTQ0Mh7FkD52KagOHFgjDHGRKZOLQ48OJIxxhhjSuMWB8YYY0xkPDiSMcYYY0pTp64KThyYSqzbdQErt55BdEw8KjlYYcG4Lqjhait2WAKOL/+C7z7B5j0XEfooAm/fJWDxlN7wquMqdlgKpFx/Bw9fwfXgMES9ioG2thYc7MuiWxcvWFqaix2aQMr1l0kdYvwSdWpx4DEO3yB/f394e3uLHYZg78lgTFm2DxP6t8T5LRNQycEKnYb/hjfvEsQODQDHV1CfPqXA0c4SEwd7ix1KjqRefw/CXqBJ4xqYPsUPE8Z2R3p6OhYs3o5PySlihwZA+vUHqEeMX5OZOORnUTVOHArg6tWr0NTUROvWrcUO5Ys8PT0xatQo0Y6/6s+z6O1dF77t6sC5vCWWBHRDMT0dbD14VbSYsuL4CsbD3RlDezdHo7qVxA4lR1Kvv/FjuqFBPTeUtSoJm3KlMLBfG8TExOPZs1dihwZA+vUHqEeM3xJOHApgw4YNGD58OC5evIjIyEixw5GklNQ0hDwIh2ctJ6FMQ0MDDWs5IejOUxEjy8DxfdvUsf6SkpIBAAYGeiJHoh71pw4xKiNzjEN+FlXjxCGfEhMTsXPnTgwePBitW7fGpk2bhMfev38PX19flCxZEvr6+nBwcMDGjRsBACkpKRg2bBgsLS2hp6cHGxsbzJs3T9h2yZIlqFy5MgwMDGBtbY0hQ4YgMTFReHzGjBmoWrWqQizLli2Dra1tjnH6+/vjwoULWL58udCs9ezZs8Kqhq+KiU1EerocJc2MFMpLmhkjOiZeZXHkhuP7tqlb/cnlhK3bT8PRoSysy1qIHY5a1J86xKgMGfLZVSHCXa44ccinXbt2wdnZGU5OTujZsyf++OMPEBEAYOrUqbh//z6OHTuG0NBQrF69GiVKlAAArFixAgcPHsSuXbsQFhaGbdu2KXzpa2hoYMWKFbh37x4CAwNx9uxZjB8/Pt9xLl++HHXq1MGAAQMQFRWFqKgoWFtb57hucnIy4uPjFRbGmOoEbj2OiIg3GDrIW+xQmIqpU4sDz6rIpw0bNqBnz54AgBYtWiAuLg4XLlyAp6cnXrx4gWrVqsHd3R0AFBKDFy9ewMHBAfXq1YNMJoONjY3CfrOORbC1tcWcOXMwaNAgrFq1Kl9xmpiYQEdHB8WKFUPp0qW/uO68efMwc+bMfB0nN+amhtDU1Mg2SOnNu3hYmBsX6rHyg+P7tqlT/QVuOYGQkEeYHNALZmbSiE0d6k8dYlQGz6r4xoWFheHatWvo3r07AEBLSwtdu3bFhg0bAACDBw/Gjh07ULVqVYwfPx5XrlwRtvX390dISAicnJwwYsQInDx5UmHfp0+fRuPGjWFlZQUjIyP06tULMTEx+PjxY5E/r4CAAMTFxQlLeHh4gfepo62Fqs7WuBAUJpTJ5XJcDPoPNSvbFXj/BcXxfdvUof6ICIFbTiD4RhgCxvvCoqSp2CEJ1KH+1CHGbw0nDvmwYcMGpKWloUyZMtDS0oKWlhZWr16NPXv2IC4uDi1btsTz58/x008/ITIyEo0bN8bYsWMBANWrV8fTp08xe/ZsJCUlwcfHB507dwYAPHv2DG3atIGbmxv27NmD4OBg/PbbbwAyxkYAGV0ZmV0imVJTUwvleenq6sLY2FhhKQxDejTC5v1XsP3wPwh7+gqj5+/Eh6Rk+Lb9oVD2X1AcX8F8TEpG2ONIhD3OGCD88tU7hD2ORFT0e5EjyyD1+gvccgJXrt7F4B/bQ09fB7FxiYiNS0RKSuG8rwtK6vUHqEeMX8NdFd+wtLQ0bN68GYsXL0azZs0UHvP29sb27dsxaNAglCxZEn5+fvDz80P9+vUxbtw4LFq0CABgbGyMrl27omvXrujcuTNatGiBd+/eITg4GHK5HIsXL4aGRkZOt2vXLoVjlCxZEq9evQIRCU1UISEhX4xZR0cH6enphVQDedexWQ28jU3E3DVHEB2TgMqOVti9YqhkmhE5voK5/zACAwPWCn8vWX8YANC2cQ3MHO0jVlgCqdffmXM3AABzF2xTKB/Qrw0a1HMTIyQFUq8/QD1i/Boxuirmz5+PgIAAjBw5EsuWLVN6O04c8ujw4cN4//49+vXrBxMTE4XHOnXqhA0bNiAyMhI1atSAq6srkpOTcfjwYVSsWBFAxqwJS0tLVKtWDRoaGvjrr79QunRpmJqawt7eHqmpqVi5ciXatm2Ly5cv4/fff1c4hqenJ968eYOFCxeic+fOOH78OI4dO/bF1gFbW1v8+++/ePbsGQwNDWFmZiYkJqoy0KchBvo0VOkx84Ljyz93twq4cWSB2GF8kZTrb8vGSWKH8FVSrr9M6hDjl6j6ktNBQUFYs2YN3NzynpxyV0UebdiwAU2aNMmWNAAZicP169ehpaWFgIAAuLm5oUGDBtDU1MSOHTsAAEZGRli4cCHc3d1Rs2ZNPHv2DEePHoWGhgaqVKmCJUuWYMGCBahUqRK2bdumMFUTACpWrIhVq1bht99+Q5UqVXDt2jWhGyQ3Y8eOhaamJlxcXFCyZEm8ePGi8CqEMcZYganyypGJiYnw9fXFunXrULx48bzHSp93mDP2/+Lj42FiYoLXMXGFNt6BScvT6A9ih/BFdhYGYofwVYfvSvvib20qlRE7BLUUHx+PUuYmiIsr2s+/zM/ZGtOOQFMv76/39E8fEDyrNcLDwxXi1NXVha6ubo7b+Pn5wczMDEuXLoWnpyeqVq2ap64KbnFgjDHGxJbfgZH/3+BgbW0NExMTYfm8tTrTjh07cOPGjVwfVwaPcWCMMcZEVtDBkTm1OHwuPDwcI0eOxKlTp6Cnl/9LmnPiwBhjjImsoIMjlZlCHxwcjOjoaFSvXl0oS09Px8WLF/Hrr78iOTkZmpqaXz0mJw6MMcaYyFQxHbNx48a4c+eOQlmfPn3g7OyMCRMmKJU0AJw4MMYYY98FIyMjVKpUSaHMwMAA5ubm2cq/hBMHxhhjTGSqvo5DQXDiwBhjjIlMrJtcnT9/Ps/bcOLAGGOMiUyd7o7JiQNjjDEmMnXqquALQDHGGGNMadziwBhjjImMuyoYY4wxpjR16qrgxIExxhgTGbc4MKYiUr+7o9TVGb9P7BC+6NWmnmKH8FV898mCkep7ODFBtXHJkM8Wh0KP5Ot4cCRjjDHGlMYtDowxxpjINGQyaOSjySE/2xQUJw6MMcaYyHhwJGOMMcaUxoMjGWOMMaY0DVnGkp/tVI0HRzLGGGNMadziwBhjjIlNls9uB6mOcTh48KDSO2zXrl2+g2GMMca+R9/c4Ehvb2+ldiaTyZCenl6QeBhjjLHvjuz//8vPdqqmVOIgl8uLOg7GGGPsu/XdDI789OlTYcXBGGOMMTWQ58QhPT0ds2fPhpWVFQwNDfHkyRMAwNSpU7Fhw4ZCD5Axxhj71mVexyE/i6rlOXH4+eefsWnTJixcuBA6OjpCeaVKlbB+/fpCDY4xxhj7HmQOjszPomp5no65efNmrF27Fo0bN8agQYOE8ipVquDBgweFGhz7dqzbdQErt55BdEw8KjlYYcG4Lqjhait2WACA4LtPsHnPRYQ+isDbdwlYPKU3vOq4ih2WQOrxjfV2w1hvN4Wyh1FxqB9wSKSIspPy6w/g+ApC6u8PZanTvSry3OLw8uVL2NvbZyuXy+VITU0tlKC+ZzKZDPv37xc7jEK192Qwpizbhwn9W+L8lgmo5GCFTsN/w5t3CWKHBgD49CkFjnaWmDjYW+xQciT1+ADgQUQsKo/cLSztfz4pdkgCqb/+OL6CUYf3hzLUqcUhz4mDi4sL/v7772zlu3fvRrVq1QolKDH5+/sr9B2Zm5ujRYsWuH37ttihqa1Vf55Fb++68G1XB87lLbEkoBuK6elg68GrYocGAPBwd8bQ3s3RqG4lsUPJkdTjA4A0uRxv4j4Jy7vEZLFDEkj99cfxFYw6vD++NXlOHKZNm4Zhw4ZhwYIFkMvl2Lt3LwYMGICff/4Z06ZNK4oYVa5FixaIiopCVFQUzpw5Ay0tLbRp0ybf+0tPT1fZlNaUlBSVHEdZKalpCHkQDs9aTkKZhoYGGtZyQtCdpyJGxgpT+VLGCFnaEf8ubI/ffvSAlVkxsUMCIP3XH8fHMn3TgyPbt2+PQ4cO4fTp0zAwMMC0adMQGhqKQ4cOoWnTpkURo8rp6uqidOnSKF26NKpWrYqJEyciPDwcb968wfnz5yGTyRAbGyusHxISAplMhmfPngEANm3aBFNTUxw8eBAuLi7Q1dXFixcvEBUVhdatW0NfXx92dnb4888/YWtri2XLluUay4QJE+Do6IhixYqhfPnymDp1qkKX0IwZM1C1alWsX78ednZ20NPTw+bNm2Fubo7kZMVffd7e3ujVq1dhVtVXxcQmIj1djpJmRgrlJc2MER0Tr9JYWNG48fgtRq6/gu6Lz2LC5msoV8IQByY1g4Ge+Fe0l/rrj+NjmdSpqyJf7+z69evj1KlThR2LJCUmJmLr1q2wt7eHubm50tt9/PgRCxYswPr162Fubg4LCwu0b98eb9++xfnz56GtrY3Ro0cjOjr6i/sxMjLCpk2bUKZMGdy5cwcDBgyAkZERxo8fL6zz6NEj7NmzB3v37oWmpiYcHBwwYsQIHDx4EF26dAEAREdH48iRIzh5Mve+5+TkZIVkIz6ePxjY1529Eyn8OzQiFjeevMX1RR3QrpYNtl98LGJkjKkPdRocme+fBNevX0doaCiAjHEPNWrUKLSgxHb48GEYGhoCAD58+ABLS0scPnwYGhrKN9CkpqZi1apVqFKlCgDgwYMHOH36NIKCguDu7g4AWL9+PRwcHL64nylTpgj/trW1xdixY7Fjxw6FxCElJQWbN29GyZIlhbIePXpg48aNQuKwdetWlCtXDp6enrkea968eZg5c6bSz1EZ5qaG0NTUyDaQ6s27eFiYGxfqsZg0xH9MxZNXCbCzMPr6ykVM6q8/jo9lkiF/96sSocEh710VERERqF+/PmrVqoWRI0di5MiRqFmzJurVq4eIiIiiiFHlvLy8EBISgpCQEFy7dg3NmzdHy5Yt8fz5c6X3oaOjAze3/01RCwsLg5aWFqpXry6U2dvbo3jx4l/cz86dO+Hh4YHSpUvD0NAQU6ZMwYsXLxTWsbGxUUgaAGDAgAE4efIkXr58CSCj+yRz4GduAgICEBcXJyzh4eFKP9/c6GhroaqzNS4EhQllcrkcF4P+Q83KdgXeP5OeYrpasLEwxOvYJLFDkfzrj+Nj6ijPiUP//v2RmpqK0NBQvHv3Du/evUNoaCjkcjn69+9fFDGqnIGBAezt7WFvb4+aNWti/fr1+PDhA9atWye0OhCRsH5O01D19fULPGjl6tWr8PX1RatWrXD48GHcvHkTkydPzjYA0sDAINu21apVQ5UqVbB582YEBwfj3r178Pf3/+LxdHV1YWxsrLAUhiE9GmHz/ivYfvgfhD19hdHzd+JDUjJ82/5QKPsvqI9JyQh7HImwxxlN7i9fvUPY40hERb8XObIMUo9vetfqqONkAesSBnC3L4GNwxtCLifs//eZ2KEBkP7rj+MrGKm/P5SlToMj89xVceHCBVy5cgVOTv8bZevk5ISVK1eifv36hRqcVMhkMmhoaCApKUn4ZR8VFSW0FoSEhHx1H05OTkhLS8PNmzeFbp1Hjx7h/fvcX9xXrlyBjY0NJk+eLJTlpdWjf//+WLZsGV6+fIkmTZrA2tpa6W0LU8dmNfA2NhFz1xxBdEwCKjtaYfeKoZJp6rz/MAIDA9YKfy9ZfxgA0LZxDcwc7SNWWAKpx2dpVgyrB9VDcUNdxCR8wrWHb9Bq9nHEJEhjSqbUX38cX8FI/f2hLHW6yVWeEwdra+scf2Gnp6ejTJkyhRKU2JKTk/Hq1SsAwPv37/Hrr78iMTERbdu2hb29PaytrTFjxgz8/PPP+O+//7B48eKv7tPZ2RlNmjTBwIEDsXr1amhra2PMmDFfbJlwcHDAixcvsGPHDtSsWRNHjhzBvn37lH4ePXr0wNixY7Fu3Tps3rxZ6e2KwkCfhhjo01DUGHLj7lYBN44sEDuMXEk9vkGrL4kdwldJ+fUHcHwFIfX3h7Ly23qgFtMxf/nlFwwfPhzXr18Xyq5fv46RI0di0aJFhRqcWI4fPw5LS0tYWlqidu3aCAoKwl9//QVPT09oa2tj+/btePDgAdzc3LBgwQLMmTNHqf1u3rwZpUqVQoMGDdChQwdhhoSenl6O67dr1w4//fQThg0bhqpVq+LKlSuYOnWq0s/DxMQEnTp1gqGhIby9vZXejjHGmOqpw1RMAJBR1s76XBQvXlwhq/nw4QPS0tKgpZXRYJH5bwMDA7x7967oov3GREREwNraGqdPn0bjxo2L5BiNGzeGq6srVqxYkedt4+PjYWJigtcxcYU23qGwPY3+IHYIaq3OeOVbsMTwalNPsUNgRUyq7+HEhHjUci6DuLii/fzL/Jz1WXsJOsUM87x9ysdE7BpYr8jjzEqproovXaCIKe/s2bNITExE5cqVERUVhfHjx8PW1hYNGjQo9GO9f/8e58+fx/nz57Fq1apC3z9jjLHCo05dFUolDn5+fkUdx3chNTUVkyZNwpMnT2BkZIS6deti27Zt0NbWLvRjVatWDe/fv8eCBQsUBrIyxhiTnm96cGRWnz59yjY1UKpN2lLQvHlzNG/eXCXHyrz8NWOMMelTpxaHPA+O/PDhA4YNGwYLCwsYGBigePHiCgtjjDHG8kZWgEXV8pw4jB8/HmfPnsXq1auhq6uL9evXY+bMmShTpozoU/4YY4wxVrTy3FVx6NAhbN68GZ6enujTpw/q168Pe3t72NjYYNu2bfD19S2KOBljjLFvljrd5CrPLQ7v3r1D+fLlAWSMZ8icflmvXj1cvHixcKNjjDHGvgPqdFvtPCcO5cuXx9OnTwFkXA1x165dADJaIkxNTQs1OMYYY+x7oE73qshz4tCnTx/cunULADBx4kT89ttv0NPTw08//YRx48YVeoCMMcbYt06dWhzyPMbhp59+Ev7dpEkTPHjwAMHBwbC3t1e4jTRjjDHGpGP16tVYvXq1MF3f1dUV06ZNQ8uWLfO0nwJdxwEAbGxsYGNjU9DdMMYYY98tVQyOLFu2LObPnw8HBwcQEQIDA9G+fXvcvHkTrq6uSu9HqcQhL/c5GDFihNLrMsYYYyz/3Q552aZt27YKf//8889YvXo1/vnnn8JPHJYuXarUzmQyGScOjDHGWB4V9MqR8fHxCuW6urrQ1dXNdbv09HT89ddf+PDhA+rUqZOnYyqVOGTOomDfp+P3o1DMMFHsMNSSq4WJ2CF8kdTvPnn4bqTYIai9NpXKiB0CU4IG8jFbIcs21tbWCuXTp0/HjBkzsq1/584d1KlTB58+fYKhoSH27dsHFxeXPB2zwGMcGGOMMSau8PBwhXtF5dba4OTkhJCQEMTFxWH37t3w8/PDhQsX8pQ8cOLAGGOMiaygXRXGxsZK3WRSR0cH9vb2AIAaNWogKCgIy5cvx5o1a5Q+JicOjDHGmMhk+bytdkGv4yCXy5GcnJynbThxYIwxxkSmkc/EIS/bBAQEoGXLlihXrhwSEhLw559/4vz58zhx4kSejsmJA2OMMSaygnZVKCM6Ohq9e/dGVFQUTExM4ObmhhMnTqBp06Z5Oma+Eoe///4ba9aswePHj7F7925YWVlhy5YtsLOzQ7169fKzS8YYY4wVoQ0bNhTKfvI8+2PPnj1o3rw59PX1cfPmTaFvJC4uDnPnzi2UoBhjjLHvSWZXRX4Wlcea1w3mzJmD33//HevWrYO2trZQ7uHhgRs3bhRqcIwxxtj34Ju+yVVYWBgaNGiQrdzExASxsbGFERNjjDH2XVHFvSoKS55bHEqXLo1Hjx5lK7906RLKly9fKEExxhhj3xONAixixJonAwYMwMiRI/Hvv/9CJpMhMjIS27Ztw9ixYzF48OCiiJExxhhjEpHnroqJEydCLpejcePG+PjxIxo0aABdXV2MHTsWw4cPL4oYGWOMsW+aKu6OWVjynDjIZDJMnjwZ48aNw6NHj5CYmAgXFxcYGhoWRXyMMcbYN08D+RzjANVnDvm+AJSOjk6e76jFvj8HD1/B9eAwRL2Kgba2Fhzsy6JbFy9YWpqLHRoA6ccHAMF3n2DznosIfRSBt+8SsHhKb3jVcRU7LAXrdl3Ayq1nEB0Tj0oOVlgwrgtquNqKHRYA6Z9jqccHSPv8qsP7Qxnq1OKQ5zEOXl5eaNSoUa4Ly2Bra4tly5YVyr42bdoEU1PTQtmXqj0Ie4EmjWtg+hQ/TBjbHenp6ViweDs+JaeIHRoA6ccHAJ8+pcDRzhITB3uLHUqO9p4MxpRl+zChf0uc3zIBlRys0Gn4b3jzLkHs0ABI/xxLPT6pn1+pvz+U9U1fx6Fq1aqoUqWKsLi4uCAlJQU3btxA5cqViyLGQufv7y9c3jPr0qJFC7FDy1HXrl3x33//iR1Gvowf0w0N6rmhrFVJ2JQrhYH92iAmJh7Pnr0SOzQA0o8PADzcnTG0d3M0qltJ7FBytOrPs+jtXRe+7erAubwllgR0QzE9HWw9eFXs0ABI/xxLPT6pn1+pvz++RXnuqli6dGmO5TNmzEBiYmKBA1KVFi1aYOPGjQplud2/XGz6+vrQ19cXO4xCkZSUcaVRAwM9kSPJmdTjk5qU1DSEPAjHT/7NhDINDQ00rOWEoDtPRYwsd1I/x1KKTx3Pr7rKuDtmfu5VUQTBfEWhTQHt2bMn/vjjj8LaXZHT1dVF6dKlFZbixYvj/Pnz0NHRwd9//y2su3DhQlhYWOD169cAAE9PTwwbNgzDhg2DiYkJSpQogalTp4KIcj3eixcv0L59exgaGsLY2Bg+Pj7C/gDg1q1b8PLygpGREYyNjVGjRg1cv34dQM5dFatXr0aFChWgo6MDJycnbNmyReFxmUyG9evXo0OHDihWrBgcHBxw8ODBglZbgcjlhK3bT8PRoSysy1qIGktOpB6fFMXEJiI9XY6SZkYK5SXNjBEdEy9SVLmT+jmWWnzqdn7VmTpdObLQEoerV69CT0/8DLmgPD09MWrUKPTq1QtxcXG4efMmpk6divXr16NUqVLCeoGBgdDS0sK1a9ewfPlyLFmyBOvXr89xn3K5HO3bt8e7d+9w4cIFnDp1Ck+ePEHXrl2FdXx9fVG2bFkEBQUhODgYEydOVLikd1b79u3DyJEjMWbMGNy9exc//vgj+vTpg3PnzimsN3PmTPj4+OD27dto1aoVfH198e7du1yfe3JyMuLj4xWWwhS49TgiIt5g6CDvQt1vYZF6fKzgpH6OpR4fKzrqNMYhz10VHTt2VPibiBAVFYXr169j6tSphRZYUTt8+HC2KaSTJk3CpEmTMGfOHJw6dQoDBw7E3bt34efnh3bt2imsa21tjaVLl0Imk8HJyQl37tzB0qVLMWDAgGzHOnPmDO7cuYOnT5/C2toaALB582a4uroiKCgINWvWxIsXLzBu3Dg4OzsDABwcHHKNfdGiRfD398eQIUMAAKNHj8Y///yDRYsWwcvLS1jP398f3bt3BwDMnTsXK1aswLVr13IdyzFv3jzMnDnza1WXL4FbTiAk5BEmB/SCmZlxkRyjIKQen1SZmxpCU1Mj20C5N+/iYWEurXqU+jmWYnzqdH7Vnez//8vPdqqW5xYHExMThcXMzAyenp44evQopk+fXhQxFgkvLy+EhIQoLIMGDQKQMdV027Zt2LNnDz59+pTjuI4ffvhB4T7oderUwcOHD5Genp5t3dDQUFhbWwtJAwC4uLjA1NQUoaGhADK+/Pv3748mTZpg/vz5ePz4ca6xh4aGwsPDQ6HMw8ND2FcmNzc34d8GBgYwNjZGdHR0rvsNCAhAXFycsISHh+e6rrKICIFbTiD4RhgCxvvCoqRpgfdZmKQen9TpaGuhqrM1LgSFCWVyuRwXg/5Dzcp2Ikb2P1I/x1KOTx3OL1O9PLU4pKeno0+fPqhcuTKKFy9eVDGphIGBAezt7XN9/MqVKwCAd+/e4d27dzAwMCjSeGbMmIEePXrgyJEjOHbsGKZPn44dO3agQ4cO+d7n510dMpkMcrk81/V1dXULfYBo4JYTuPrPPYwa0Rl6+jqIjcsYQFtMXxc6Ojl3xaiS1OMDgI9JyQiPjBH+fvnqHcIeR8LYSB+WFuK/D4f0aIQhM7egWsVyqO5qi9Xbz+FDUjJ82/4gdmgApH+OpR6f1M+v1N8fyspvt4Pkuyo0NTXRrFkzhIaGqn3i8CWPHz/GTz/9hHXr1mHnzp3w8/PD6dOnoaHxvwaaf//9V2Gbf/75Bw4ODtDU1My2v4oVKyI8PBzh4eFCq8P9+/cRGxurcBEtR0dHODo64qeffkL37t2xcePGHBOHihUr4vLly/Dz8xPKLl++LMkLcp05l3Gr9bkLtimUD+jXBg3queW0iUpJPT4AuP8wAgMD1gp/L1l/GADQtnENzBztI1ZYgo7NauBtbCLmrjmC6JgEVHa0wu4VQyXTlC31cyz1+KR+fqX+/lDWN5s4AEClSpXw5MkT2NmpdzNVcnIyXr1SnCetpaWF4sWLo2fPnmjevDn69OmDFi1aoHLlyli8eDHGjRsnrPvixQuMHj0aP/74I27cuIGVK1di8eLFOR6rSZMmqFy5Mnx9fbFs2TKkpaVhyJAhaNiwIdzd3ZGUlIRx48ahc+fOsLOzQ0REBIKCgtCpU6cc9zdu3Dj4+PigWrVqaNKkCQ4dOoS9e/fi9OnThVdBhWTLxklih/BFUo8PANzdKuDGkQVih/FFA30aYqBPQ7HDyJHUz7HU4wOkfX7V4f2hjMzrCeVnO1XLc+IwZ84cjB07FrNnz0aNGjWyNeEbG0sjC/2a48ePw9LSUqHMyckJPXr0wPPnz3H4cEbWamlpibVr16J79+5o1qwZqlSpAgDo3bs3kpKSUKtWLWhqamLkyJEYOHBgjseSyWQ4cOAAhg8fjgYNGkBDQwMtWrTAypUrAWS05MTExKB37954/fo1SpQogY4dO+Y6UNHb2xvLly/HokWLMHLkSNjZ2WHjxo3w9PQspNphjDGmSurU4iCjL118IItZs2ZhzJgxMDL633zerJkOEUEmk+U4OPBb4+npiapVqxbaJaWlKj4+HiYmJgj8+wGKGRp9fQOWjauFidghfJGdRdGO3Smow3cjxQ5B7bWpVEbsEL7oafQHsUPIUWJCPGo5l0FcXFyR/iDO/JydcyQEegZ5/5z99CEBU1pXLfI4s1K6xWHmzJkYNGhQtmsFMMYYY6xg1OkmV0onDpkNEw0bSrOfizHGGFNXGrJ83lZb6mMcxBiEIUXnz58XOwTGGGPfEHUa45CnxMHR0fGrycOXLmnMGGOMsRzk974TUk8cZs6cCRMTaQ/2YowxxljRyVPi0K1bN1hYiH/HNsYYY+xbogEZNPLRfJCfbQpK6cSBxzcwxhhjReObnlXBGGOMscL1TQ6O/NLNkRhjjDGWf+o0HTPPt9VmjDHG2Pcrz/eqYIwxxljh+ibHODDGGGOsaGggn10VUp5VwRhjjLGiwS0OjDHGGFOaBvI36FCMgYqcOLCvauFiqbLbtX5rpH5baKnfVlvqt4QGpH+OGStsnDgwxhhjIpPJZPm60KIYF2fkxIExxhgTmQz5u1+VGNd05sSBMcYYE5k6XQCKEwfGGGNMAtTljlB85UjGGGOMKY1bHBhjjDGR8XUcGGOMMaY0dZpVwV0VjDHGmMg0CrAoa968eahZsyaMjIxgYWEBb29vhIWF5StWxhhjjIkos8UhP4uyLly4gKFDh+Kff/7BqVOnkJqaimbNmuHDhw95ipW7KhhjjLHvwPHjxxX+3rRpEywsLBAcHIwGDRoovR9OHBhjjDGRiXEBqLi4OACAmZlZnrbjxIExxhgTWUEHR8bHxyuU6+rqQldXN9ft5HI5Ro0aBQ8PD1SqVClPx+QxDowxxpjICjo40traGiYmJsIyb968Lx5v6NChuHv3Lnbs2JHnWLnFIZ88PT1RtWpVLFu2TOltZDIZ9u3bB29v7yI/ltSs23UBK7eeQXRMPCo5WGHBuC6o4WordlgCqcZ38PAVXA8OQ9SrGGhra8HBviy6dfGCpaW52KEpkGr9ZZJyfOpwjqVcf8F3n2DznosIfRSBt+8SsHhKb3jVcRU7LJULDw9XuIvxl1obhg0bhsOHD+PixYsoW7Zsno/FLQ5f4e/vn+Mo1oULF2L27NmFeqzz589DJpMhNjZWoXzv3r2FfixV2nsyGFOW7cOE/i1xfssEVHKwQqfhv+HNuwSxQwMg7fgehL1Ak8Y1MH2KHyaM7Y709HQsWLwdn5JTxA5NIOX6A6Qfn9TPsdTr79OnFDjaWWLiYG+xQymQgs6qMDY2VlhyShyICMOGDcO+fftw9uxZ2NnZ5StWThyU0KJFC0RFRSksNWrUgJGRkUqOb2ZmprJjFYVVf55Fb++68G1XB87lLbEkoBuK6elg68GrYocGQNrxjR/TDQ3quaGsVUnYlCuFgf3aICYmHs+evRI7NIGU6w+QfnxSP8dSrz8Pd2cM7d0cjermrZ9eamQFWJQ1dOhQbN26FX/++SeMjIzw6tUrvHr1CklJSXmKlRMHJejq6qJ06dIKS+PGjTFq1ChhnaioKLRu3Rr6+vqws7PDn3/+CVtb22zdC2/fvkWHDh1QrFgxODg44ODBgwCAZ8+ewcvLCwBQvHhxyGQy+Pv7A8joqsh6LFtbW8ydOxd9+/aFkZERypUrh7Vr1yoc58qVK6hatSr09PTg7u6O/fv3QyaTISQkpLCr54tSUtMQ8iAcnrWchDINDQ00rOWEoDtPVRpLTqQe3+eSkpIBAAYGeiJHkkHq9Sf1+HIipXOsjvWnrjIvOZ2fRVmrV69GXFwcPD09YWlpKSw7d+7MU6ycOBSS3r17IzIyEufPn8eePXuwdu1aREdHZ1tv5syZ8PHxwe3bt9GqVSv4+vri3bt3sLa2xp49ewAAYWFhiIqKwvLly3M93uLFi+Hu7o6bN29iyJAhGDx4sHAFsPj4eLRt2xaVK1fGjRs3MHv2bEyYMKFonvhXxMQmIj1djpJmii0mJc2MER0Tn8tWqiP1+LKSywlbt5+Go0NZWJe1EDscANKvP6nH9zmpnWN1qz91pgFZvhdlEVGOS+aPVOVjZV91+PBhGBoaCkuXLl0UHn/w4AFOnz6NdevWoXbt2qhevTrWr1+fY/OPv78/unfvDnt7e8ydOxeJiYm4du0aNDU1hbm0FhYWKF26NExMTHKNqVWrVhgyZAjs7e0xYcIElChRAufOnQMA/Pnnn5DJZFi3bh1cXFzQsmVLjBs37qvPMzk5GfHx8QoLk47ArccREfEGQwd5ix0KKyJ8jpk64MRBCV5eXggJCRGWFStWKDweFhYGLS0tVK9eXSizt7dH8eLFs+3Lzc1N+LeBgQGMjY1zbJn4mqz7kclkKF26tLCfsLAwuLm5QU/vf02dtWrV+uo+582bpzCdx9raOs9xfc7c1BCamhrZBlK9eRcPC3PjXLZSHanHlylwywmEhDxCwARfmJlJJy6p15/U48tKiudYnepP3amiq6KwcOKgBAMDA9jb2wuLpaVlvvelra2t8LdMJoNcLhdtP1kFBAQgLi5OWMLDwwu0PwDQ0dZCVWdrXAj6341U5HI5Lgb9h5qV8zeitzBJPT4iQuCWEwi+EYaA8b6wKGkqdkgKpF5/Uo8PkPY5Vof6+1bICvCfqvF1HAqBk5MT0tLScPPmTdSoUQMA8OjRI7x//z5P+9HR0QEApKenFzierVu3Ijk5WZiSExQU9NXtvnalsfwa0qMRhszcgmoVy6G6qy1Wbz+HD0nJ8G37Q6EfKz+kHF/glhO4+s89jBrRGXr6OoiNSwQAFNPXhY6O9le2Vg0p1x8g/fikfo6lXn8fk5IRHhkj/P3y1TuEPY6EsZE+LC2yt/pKVX5bD8RoceDEoRA4OzujSZMmGDhwIFavXg1tbW2MGTMG+vr6ebqEqI2NDWQyGQ4fPoxWrVpBX18fhoaGeY6nR48emDx5MgYOHIiJEyfixYsXWLRoEQBx7t3esVkNvI1NxNw1RxAdk4DKjlbYvWKoZJo6pRzfmXM3AABzF2xTKB/Qrw0a1HPLaROVk3L9AdKPT+rnWOr1d/9hBAYG/G9W2ZL1hwEAbRvXwMzRPmKFlWeyPA50zLqdqnHiUEg2b96Mfv36oUGDBihdujTmzZuHe/fuKYwz+BorKyvMnDkTEydORJ8+fdC7d29s2rQpz7EYGxvj0KFDGDx4MKpWrYrKlStj2rRp6NGjR57iKUwDfRpioE9DUY6tDKnGt2XjJLFDUIpU6y+TlONTh3Ms5fpzd6uAG0cWiB3Gd0VGRCR2EN+iiIgIWFtb4/Tp02jcuLHY4WDbtm3o06cP4uLioK+vr9Q28fHxMDExweuYOIVLmTLlHb4bKXYIX9SmUhmxQ1B7fI4L5mn0B7FDyFFiQjxqOZdBXFzRfv5lfs7u+fcxDAzzfqG/D4kJ6FS7QpHHmRW3OBSSs2fPIjExEZUrV0ZUVBTGjx8PW1vbPN3jvDBt3rwZ5cuXh5WVFW7duoUJEybAx8dH6aSBMcaY6vAYh+9QamoqJk2ahCdPnsDIyAh169bFtm3bss1+UJVXr15h2rRpePXqFSwtLdGlSxf8/PPPosTCGGPsy/I7Q4LHOKix5s2bo3nz5mKHIRg/fjzGjx8vdhiMMcaUoCHLWPKznarxdRwYY4wxpjRucWCMMcZExl0VjDHGGFMaD45kjDHGmNJkyF/rgQh5AycOjDHGmNh4cCRjjDHGvknc4sAYY4yJjAdHMsYYY0xpPDiSMcYYY0qTIX8DHXlwJGOMMfYd0oAMGvloPsjPrbgLigdHMsYYY0xp3OLAvur4/SgUM0wUO4wcSf2Wwa4WJmKH8EVSvyU0K7jiNYeJHcIXvQ/6VewQchSvl67S43FXBWOMMcaUp0aZAycOjDHGmMh4OiZjjDHGlJfP6ZhitDjw4EjGGGOMKY1bHBhjjDGRqdEQB04cGGOMMdGpUebAiQNjjDEmMh4cyRhjjDGlqdO9KnhwJGOMMcaUxi0OjDHGmMjUaIgDJw6MMcaY6NQoc+DEgTHGGBMZD45kjDHGmNLUaXAkJw55dP78eXh5eeH9+/cwNTVVaptnz57Bzs4ON2/eRNWqVQttv+rg4OEruB4chqhXMdDW1oKDfVl06+IFS0tzsUNTsG7XBazcegbRMfGo5GCFBeO6oIarrdhhAQCC7z7B5j0XEfooAm/fJWDxlN7wquMqdlgCqZ9jjq/gLEuaYMbw9mhSxxX6etp4GvEWQ2dtRUjoC7FDE0j5PfytEXVWhb+/P2QyGebPn69Qvn//fsjESKPyaM+ePdDU1MTLly9zfNzBwQGjR4+GtbU1oqKiUKlSJRVHKL4HYS/QpHENTJ/ihwljuyM9PR0LFm/Hp+QUsUMT7D0ZjCnL9mFC/5Y4v2UCKjlYodPw3/DmXYLYoQEAPn1KgaOdJSYO9hY7lBxJ/RxzfAVjYqSP4+tHIzVNji4jV+GHrj9jyrK9iI3/KHZoAqm/h5UhK8CiaqJPx9TT08OCBQvw/v37QttnSopq3nDt2rWDubk5AgMDsz128eJFPHr0CP369YOmpiZKly4NLa3vr4Fn/JhuaFDPDWWtSsKmXCkM7NcGMTHxePbsldihCVb9eRa9vevCt10dOJe3xJKAbiimp4OtB6+KHRoAwMPdGUN7N0ejutJMPKV+jjm+ghnl1xQvX7/HsFlbceP+c7yIjMG5fx/g2cu3YocmkPp7WClqlDmInjg0adIEpUuXxrx583JdZ8+ePXB1dYWuri5sbW2xePFihcdtbW0xe/Zs9O7dG8bGxhg4cCA2bdoEU1NTHD58GE5OTihWrBg6d+6Mjx8/IjAwELa2tihevDhGjBiB9PR0YV9btmyBu7s7jIyMULp0afTo0QPR0dE5xqWtrY1evXph06ZN2R77448/ULt2bbi6uuLZs2eQyWQICQkRHj969CgcHR2hr68PLy8vPHv2LNs+Ll26hPr160NfXx/W1tYYMWIEPnz4IDz+/v179O7dG8WLF0exYsXQsmVLPHz4UHj8+fPnaNu2LYoXLw4DAwO4urri6NGjudazKiQlJQMADAz0RI0jU0pqGkIehMOzlpNQpqGhgYa1nBB056mIkakvqZ3jz3F8edOifmXcDH2BjfP64r8T83Bh6wT09q4rdliCb+U9LCvAf6omeuKgqamJuXPnYuXKlYiIiMj2eHBwMHx8fNCtWzfcuXMHM2bMwNSpU7N9WS9atAhVqlTBzZs3MXXqVADAx48fsWLFCuzYsQPHjx/H+fPn0aFDBxw9ehRHjx7Fli1bsGbNGuzevVvYT2pqKmbPno1bt25h//79ePbsGfz9/XONv1+/fnj48CEuXrwolCUmJmL37t3o169fjtuEh4ejY8eOaNu2LUJCQtC/f39MnDhRYZ3Hjx+jRYsW6NSpE27fvo2dO3fi0qVLGDZsmLCOv78/rl+/joMHD+Lq1asgIrRq1QqpqakAgKFDhyI5ORkXL17EnTt3sGDBAhgaGub6XIqaXE7Yuv00HB3KwrqshWhxZBUTm4j0dDlKmhkplJc0M0Z0TLxIUakvKZ7jrDi+vLO1KoG+nerjSfgbdBr+G/7Ycwnzx3RGt9a1xQ4NwLfzHs4cHJmfRdUk0XbeoUMHVK1aFdOnT8eGDRsUHluyZAkaN24sJAOOjo64f/8+fvnlF4Uv9EaNGmHMmDHC33///TdSU1OxevVqVKhQAQDQuXNnbNmyBa9fv4ahoSFcXFzg5eWFc+fOoWvXrgCAvn37CvsoX748VqxYgZo1ayIxMTHHL10XFxf88MMP+OOPP9CgQQMAwK5du0BE6NatW47PNzOmzJYTJycn4Ys907x58+Dr64tRo0YByBgvsWLFCjRs2BCrV69GeHg4Dh48iMuXL6Nu3Yzsf9u2bbC2tsb+/fvRpUsXvHjxAp06dULlypWF5/MlycnJSE5OFv6Ojy/cN13g1uOIiHiDqZN6Fep+mXRI/RxzfHmnoSFDSOgLzF51CABw578IVCxviT4d62HHkX9Fjo6JQfQWh0wLFixAYGAgQkNDFcpDQ0Ph4eGhUObh4YGHDx8qdDG4u7tn22exYsWEpAEASpUqBVtbW4UEoFSpUgpdEcHBwWjbti3KlSsHIyMjNGzYEADw4kXuo4f79u2L3bt3IyEhYyDOH3/8gS5dusDIyCjH9UNDQ1G7tmK2XqdOHYW/b926hU2bNsHQ0FBYmjdvDrlcjqdPnyI0NBRaWloK+zE3N4eTk5NQhyNGjMCcOXPg4eGB6dOn4/bt27k+ByAjWTExMREWa2vrL66fF4FbTiAk5BECJvjCzMy40PZbUOamhtDU1Mg2iOrNu3hYmEsnTnUg1XOciePLn9dv4/HgieJ4i/+evULZ0sVFikjRt/IeVqMhDtJJHBo0aIDmzZsjICAgX9sbGBhkK9PW1lb4WyaT5Vgml8sBAB8+fEDz5s1hbGyMbdu2ISgoCPv27QPw5QGXmS0Lu3btwsOHD3H58uVcuymUlZiYiB9//BEhISHCcuvWLTx8+FAhGfqS/v3748mTJ+jVqxfu3LkDd3d3rFy5Mtf1AwICEBcXJyzh4eEFeg4AQEQI3HICwTfCEDDeFxYlTQu8z8Kko62Fqs7WuBAUJpTJ5XJcDPoPNSvbiRiZ+pD6Oeb4CubfW0/gYKPYbVKhnAUiXr0TKSJF38x7WI0yB8kkDgAwf/58HDp0CFev/m8kbMWKFXH58mWF9S5fvgxHR0doamoW6vEfPHiAmJgYzJ8/H/Xr14ezs3OuAyOzMjIyQpcuXfDHH39g48aNcHR0RP369XNdv2LFirh27ZpC2T///KPwd/Xq1XH//n3Y29tnW3R0dFCxYkWkpaXh33//11QYExODsLAwuLi4CGXW1tYYNGgQ9u7dizFjxmDdunW5xqWrqwtjY2OFpaACt5zAlat3MfjH9tDT10FsXCJi4xKRkpJa4H0XliE9GmHz/ivYfvgfhD19hdHzd+JDUjJ82/4gdmgAgI9JyQh7HImwx5EAgJev3iHscSSiogtvJlJBSP0cc3wFs2r7WbhXtsNo/2awK1sCnZu7w6+DB9b/dfHrG6uI1N/DylDV4MiLFy+ibdu2KFOmDGQyGfbv35/nWCUxxiFT5cqV4evrixUrVghlY8aMQc2aNTF79mx07doVV69exa+//opVq1YV+vHLlSsHHR0drFy5EoMGDcLdu3cxe/Zspbbt168f6tevj9DQUEyYMOGL6w4aNAiLFy/GuHHj0L9/fwQHB2cb7DlhwgT88MMPGDZsGPr37w8DAwPcv38fp06dwq+//goHBwe0b98eAwYMwJo1a2BkZISJEyfCysoK7du3BwCMGjUKLVu2hKOjI96/f49z586hYsWK+aqb/Dpz7gYAYO6CbQrlA/q1QYN6biqNJTcdm9XA29hEzF1zBNExCajsaIXdK4ZKppnz/sMIDAxYK/y9ZP1hAEDbxjUwc7SPWGEJpH6OOb6CuXn/BXqNW4dpQ9thXP+WeB4Zg0lL9uCv49fFDk0g9fewMlR15cgPHz6gSpUq6Nu3Lzp27Jj3A0JiiQMAzJo1Czt37hT+rl69Onbt2oVp06Zh9uzZsLS0xKxZs7440yG/SpYsiU2bNmHSpElYsWIFqlevjkWLFqFdu3Zf3bZevXpwcnLCo0eP0Lt37y+uW65cOezZswc//fQTVq5ciVq1amHu3LkKAzPd3Nxw4cIFTJ48GfXr1wcRoUKFCsIgTgDYuHEjRo4ciTZt2iAlJQUNGjTA0aNHhe6Y9PR0DB06FBERETA2NkaLFi2wdOnSfNZO/mzZOEmlx8uvgT4NMdCnodhh5MjdrQJuHFnw9RVFIvVzzPEV3IlLd3Hi0l2xw/giKb+HpaRly5Zo2bJlgfYhIyIqpHjYNyY+Ph4mJiYI/PsBihnmPNBTbG0qlRE7hC96Gv3h6yuJ6F50nNghsCLWq89csUP4ovdBv4odQo7i4+NRytwEcXFxhdJt+6XjmJiY4NqDSBga5f04iQnxqOVcBuHh4Qpx6urqQldX94vbymQy7Nu3D97e3nk6pqTGODDGGGPfpQIOjrS2tlaYFfeliyoWlOS6KhhjjLHvTUFvq51Ti0NR4cSBMcYYE1lBB0cW1kw4ZXBXBWOMMcaUxi0OjDHGmMjyey2nvG6TmJiIR48eCX8/ffoUISEhMDMzQ7ly5ZTaBycOjDHGmNhUlDlcv34dXl5ewt+jR48GAPj5+eV4p+eccOLAGGOMiayggyOV5enpiYJehYETB8YYY0xs+b1F9vd+rwrGGGOMSRu3ODDGGGMiU9XgyMLAiQNjjDEmNjXKHDhxYIwxxkSmqsGRhYETB8YYY0xkqrqtdmHgwZGMMcYYUxq3OLCvqljSOF+3e1UFqd+2WupcLUzEDuGL1OG231KvwxtHFogdwhcdvhspdgg5+piYoNLjqdEQB04cGGOMMdGpUebAiQNjjDEmMh4cyRhjjDGlyZDPwZGFHsnX8eBIxhhjjCmNWxwYY4wxkanREAdOHBhjjDGxqdN1HDhxYIwxxkSnPm0OnDgwxhhjIlOnFgceHMkYY4wxpXGLA2OMMSYy9emo4MSBMcYYE506dVVw4sAYY4yJjK8cyRhjjDHlqVFfBScOSpLJZNi3bx+8vb1VfmxPT09UrVoVy5YtU/mxC0Pw3SfYvOciQh9F4O27BCye0htedVzFDkvA8RWM1OM7ePgKrgeHIepVDLS1teBgXxbdunjB0tJc7NAASL/+pB6f1M/vt4hnVfy/N2/eYPDgwShXrhx0dXVRunRpNG/eHJcvX1ZZDOfPn4dMJkNsbKxC+d69ezF79myVxVHYPn1KgaOdJSYO9hY7lBxxfAUj9fgehL1Ak8Y1MH2KHyaM7Y709HQsWLwdn5JTxA4NgPTrT+rxSf38KktWgEXVuMXh/3Xq1AkpKSkIDAxE+fLl8fr1a5w5cwYxMTFihwYzMzOxQygQD3dneLg7ix1Grji+gpF6fOPHdFP4e2C/Nhg6cjmePXsFZ6dyIkX1P1KvP6nHJ/Xzqyx1GhzJLQ4AYmNj8ffff2PBggXw8vKCjY0NatWqhYCAALRr105Y7+3bt+jQoQOKFSsGBwcHHDx4UGE/Fy5cQK1ataCrqwtLS0tMnDgRaWlpwuPJyckYMWIELCwsoKenh3r16iEoKAgA8OzZM3h5eQEAihcvDplMBn9/fwAZXRWjRo0S9mNra4u5c+eib9++MDIyQrly5bB27VqFWK5cuYKqVatCT08P7u7u2L9/P2QyGUJCQgqx5hhTP0lJyQAAAwM9kSNhRUFdz6+sAP+pGicOAAwNDWFoaIj9+/cjOTk51/VmzpwJHx8f3L59G61atYKvry/evXsHAHj58iVatWqFmjVr4tatW1i9ejU2bNiAOXPmCNuPHz8ee/bsQWBgIG7cuAF7e3s0b94c7969g7W1Nfbs2QMACAsLQ1RUFJYvX55rLIsXL4a7uztu3ryJIUOGYPDgwQgLCwMAxMfHo23btqhcuTJu3LiB2bNnY8KECYVRVYypNbmcsHX7aTg6lIV1WQuxw2GFTK3Prxr1VXDiAEBLSwubNm1CYGAgTE1N4eHhgUmTJuH27dsK6/n7+6N79+6wt7fH3LlzkZiYiGvXrgEAVq1aBWtra/z6669wdnaGt7c3Zs6cicWLF0Mul+PDhw9YvXo1fvnlF7Rs2RIuLi5Yt24d9PX1sWHDBmhqagpdEhYWFihdujRMTExyjblVq1YYMmQI7O3tMWHCBJQoUQLnzp0DAPz555+QyWRYt24dXFxc0LJlS4wbN+6r9ZCcnIz4+HiFhbFvSeDW44iIeIOhg7zFDoUVAT6/qsGJw//r1KkTIiMjcfDgQbRo0QLnz59H9erVsWnTJmEdNzc34d8GBgYwNjZGdHQ0ACA0NBR16tSBLEuHk4eHBxITExEREYHHjx8jNTUVHh4ewuPa2tqoVasWQkND8xxv1lhkMhlKly4txBIWFgY3Nzfo6f2vqa5WrVpf3ee8efNgYmIiLNbW1nmOizGpCtxyAiEhjxAwwRdmZsZih8MKmbqfXzVqcODEISs9PT00bdoUU6dOxZUrV+Dv74/p06cLj2trayusL5PJIJfLVR1mkcUSEBCAuLg4YQkPDy/Q/hiTAiJC4JYTCL4RhoDxvrAoaSp2SKwQfSvnN3NwZH4WVeNZFV/g4uKC/fv3K7VuxYoVsWfPHhCR0Opw+fJlGBkZoWzZsjA3N4eOjg4uX74MGxsbAEBqaiqCgoKEgY86OjoAgPT09ALF7eTkhK1btyI5ORm6uroAIAzC/BJdXV1h/cL0MSkZ4ZH/m53y8tU7hD2OhLGRPiwtihf68fKK4ysYqccXuOUErv5zD6NGdIaevg5i4xIBAMX0daGjo/2VrYue1OtP6vFJ/fwqL78DHfnKkaKIiYlBly5d0LdvX7i5ucHIyAjXr1/HwoUL0b59e6X2MWTIECxbtgzDhw/HsGHDEBYWhunTp2P06NHQ0NCAgYEBBg8ejHHjxsHMzAzlypXDwoUL8fHjR/Tr1w8AYGNjA5lMhsOHD6NVq1bQ19eHoaFhnp9Pjx49MHnyZAwcOBATJ07EixcvsGjRIgBQ6EpRlfsPIzAw4H+zPpasPwwAaNu4BmaO9lF5PJ/j+ApG6vGdOXcDADB3wTaF8gH92qBBPbecNlEpqdef1OOT+vlVljpNx+TEARmzKmrXro2lS5cKYxGsra0xYMAATJo0Sal9WFlZ4ejRoxg3bhyqVKkCMzMz9OvXD1OmTBHWmT9/PuRyOXr16oWEhAS4u7vjxIkTKF68uLCPmTNnYuLEiejTpw969+6tMMZCWcbGxjh06BAGDx6MqlWronLlypg2bRp69OihMO5BVdzdKuDGkQUqP66yOL6CkXp8WzYq9x4Wi9TrT+rxSf38fotkRERiB8GK3rZt29CnTx/ExcVBX19fqW3i4+NhYmKCaw8iYWikfoONmPq7Fx0ndghf5WqR++wn9nVSPccfExPgV98ZcXFxMDYuus+/zM/ZZ1Hv8nWc+Ph42FqaFXmcWXGLwzdq8+bNKF++PKysrHDr1i1MmDABPj4+SicNjDHGVIe7KpjoXr16hWnTpuHVq1ewtLREly5d8PPPP4sdFmOMsRzwbbWZ6MaPH4/x48eLHQZjjDElqFOLA1/HgTHGGGNK4xYHxhhjTGT5vQok31abMcYY+x6pUebAiQNjjDEmMh4cyRhjjDGl8eBIxhhjjH2TOHFgjDHGRKbK22r/9ttvsLW1hZ6eHmrXro1r167laXtOHBhjjDGxqShz2LlzJ0aPHo3p06fjxo0bqFKlCpo3b47o6Gil98GJA2OMMSYyWQH+y4slS5ZgwIAB6NOnD1xcXPD777+jWLFi+OOPP5TeBycOjDHGmMgyB0fmZ1FWSkoKgoOD0aRJE6FMQ0MDTZo0wdWrV5XeD8+qYLnKvHFqYmKCyJGw79VHNXjtJeqLcQmeb4dUz3HSh0QA//scLGrx8fEF2u7z7XV1daGrq6tQ9vbtW6Snp6NUqVIK5aVKlcKDBw+UPiYnDixXCQkZb+hG7k4iR8IYY+JISEiAiUnR3TpdR0cHpUuXhoOddb73YWhoCGtrxe2nT5+OGTNmFDC6nHHiwHJVpkwZhIeHw8jICLJCmCwcHx8Pa2trhIeHq+y+8XnB8RUMx1cwHF/BFHZ8RISEhASUKVOmEKLLnZ6eHp4+fYqUlJR874OIsn1Gf97aAAAlSpSApqYmXr9+rVD++vVrlC5dWunjceLAcqWhoYGyZcsW+n6NjY0l+cGTieMrGI6vYDi+ginM+IqypSErPT096OnpFflxdHR0UKNGDZw5cwbe3t4AALlcjjNnzmDYsGFK74cTB8YYY+w7MXr0aPj5+cHd3R21atXCsmXL8OHDB/Tp00fpfXDiwBhjjH0nunbtijdv3mDatGl49eoVqlatiuPHj2cbMPklnDgwldHV1cX06dNz7HuTAo6vYDi+guH4Ckbq8UnJsGHD8tQ18TkZqWquCWOMMcbUHl8AijHGGGNK48SBMcYYY0rjxIExxhhjSuPEgTHGGGNK48SBMcYYY0rjxIFJSnp6usL/pUqqk5HkcrnC/wEgKSlJrHCUljVeQLr1K0Wf1x3LH65H5XHiwCTh5cuXuH37NjQ1NbF3714EBgZKKnnI/CJLTk4GgEK5d0dR0NDQwPPnz/Hbb78BAHbt2oUmTZrgw4cPIkf2ZRoaGR9F+/btQ2RkpGTrN6cvF7GTnMy6Cw0NFTWOwpZZry9fvsTbt2+L/HiZ9fjs2bMiP5a648SBiS4hIQHDhg3DxIkTsXjxYnTu3Bm6urrQ1NQUOzQA/7uBzLFjx9C/f3+0adMGJ06cQExMjNihZSOXy7F06VKsXbsW/v7+6NmzJ/r37w8DAwOxQ/uqkJAQjBkzBmFhYQCk1+okl8sVvqRv3rwJQLwkMmsS8/fff8PV1RX79u0TJZbClvmeO3DgALp06YIzZ87k+7bTX5O1Hk+cOIHy5cvj3LlzRXKsbwYxJgEHDx6kypUrk0wmozlz5hARUXp6ushR/c+5c+dIR0eH+vfvT3Xr1qWyZcvSjBkzKCIiQuzQiIhox44ddP78eeHvli1bkkwmo27dugllUqrP3Hh6elLLli3FDuOLxo0bR1ZWVmRsbEx169aly5cvU2pqqkpjkMvlwr9XrlxJy5YtI5lMRiVKlKBdu3apNJaisn//fjIwMKAFCxZQeHh4kRwj63ti7dq19Ouvv5JMJiMbGxs6depUkRzzW8AtDkxUmdl+9erVkZaWBgcHB9y8eRMhISHQ0NAQvRkYAF69eoWTJ09i8eLFWLduHS5fvoyBAwdi586dWLt2LV6+fClqfI8ePcLSpUvx888/48KFCwAy7hDYqFEjPH/+HL/88gs+fvwIDQ0NyfyKzzzvmec385bCM2fOREREBM6ePStabJ/L+ot03759OHDgAFavXo0TJ04gNTUVgwYNwpkzZ5CWlqaymDJbOaZOnYpZs2bBwsICy5cvR+PGjdGrVy/s2rVLZbEUhaioKEydOhVz5szB+PHjYWFhgYSEBJw8eRLBwcGFdpzMFqTJkydjypQpMDAwwNy5c+Hk5ITOnTvj1KlThXasb4rYmQv7fmX+anrx4gUlJSXRixcv6MCBA9SoUSNq164d3bx5U2G9lJQUlcd49+5dcnR0JHt7ewoMDFR4bNasWeTs7EwzZsygFy9eqDy2rA4cOECtWrWiFi1a0L1794Ty/v37U82aNWnhwoX04cMHoTwuLk6MMIlI8dfy0aNHieh/v/xevnxJ1atXp4kTJ4oS25fs3LmT5s+fT0uXLhXK0tLSqH79+lSpUiU6ceKESlseoqOjqVKlSrR27Vqh7NOnTzR8+HDS09Oj3bt3qyyWwhYdHU0//PAD7dy5k6Kjo2nmzJnUoEEDMjMzIwcHB9q+fXuhHSsiIoIcHBxo8+bNQll8fDx1796dihcvTqdPny60Y30rOHFgosj88jhw4AA5ODjQ1q1bhcd27txJjRo1Im9vbyF5mDdvHm3cuFHhS0dVBg8eTFpaWjRixAiKjY1VeOznn38mCwsLmjt3LqWlpak8tqzHPHDgADVt2pRatGhBFy5cIKKML5IBAwZQ7dq1acGCBZSYmEhTp06lBg0aiJKIZW0aDgoKIisrK3Jzc6PZs2fTo0ePiIhoz549ZGFhQf/++6/K48tNcnIymZmZkUwmo+HDhys8lp6eTg0bNqQqVarQ/v37VfY6iIiIIDMzM6FrIj09neRyOcXGxlKtWrXI3Nyc9u3bJzymTt68eUM//PAD1a9fnwwNDaljx460YsUKCgkJoUaNGtGUKVMK7ViPHz8mU1NTOnnyJBH9r65ev35NLi4uZGNjQ+fOnSMiEuXzR4o4cWCi2bdvHxUrVoyWLl2q8CuZiOivv/6ipk2bkqurK/n4+JBMJhOSiKKU2wfDkCFDyMbGhlatWpUtefjll1+ELz1V+zzevXv3UvPmzalFixZ08eJFIsr40hsyZAi5urqSs7MzlSxZkq5evSpqrAMGDKBGjRrR+/fvaezYsdSyZUsyMDCg2bNn07p166hHjx60cuVKIiJRErKcXgcJCQlUtWpVcnBwoKCgIIV10tPTycXFhXr27Fkk8eT2xd+uXTtq2LAhvXv3TlhPLpdTz549yc3NjXR1dSkkJKRIYiosmfUYHh5OT58+FVrvoqKiaP369bRmzRqKj48X1m/Xrh1NmzYtX8fKrR7r169P7dq1o+TkZCGmlJQU8vb2Jnt7ezI1NaXnz58rxPs948SBieLNmzfk7u5Ov/zyCxERpaam0ocPH2jPnj3CQKgLFy7Q5MmTqVu3bnT37t0ijynzAyEoKIhWrVpF69evF36FEGU0+5cvXz7H5EFMFy5coB9//FH4O6fkISUlhQ4ePEgbNmyghw8fqjzGrB+29+/fp9q1aysMPktKSqJVq1ZR+/btydnZmWQyGbm4uAgf5KqU9cslISGBkpKShDji4uLIzs6OatasmS2RlcvlRZLkZI0nLCyMQkNDhb8PHDhAderUob59+9LHjx+JKONcd+jQgc6dOyd0X3369EmSX3iZMe3Zs4ccHByofPnypK+vTyNGjKD79+8rrJuYmEgTJ06kEiVKUFhYWJ6PlbUenzx5Qs+ePRP+3rp1K9WsWZNGjBghlKWkpFDnzp3p77//Jg8PD/Lx8REliZUiThyYKJ49e0a2trZ04cIFio+Pp5kzZ1L9+vVJU1OTXFxcFPoVVdlvvGfPHjI2NqZ69eqRnZ0dlSlThkaOHCk8PmDAAHJycqLFixeLOk4gU2pqKq1cuZJKlixJw4YNE8qzJg9///23iBEq2rBhA7Vu3Zp69uxJaWlp2c7t69ev6c6dO9S7d28qV64cLV++XKXxZf1ynTt3LrVq1YqcnJxoxIgRdPz4cSIiio2NJTs7O6pVq1aOv+aL6stl/Pjx5ODgQHp6euTr6yt05axZs4Zq1qxJdnZ21KdPH6patSq5ublReno6DR8+nJo0aVIk8RSW8+fPU7FixWjlypV05coV2rx5Mzk5OVGPHj3oxo0bRES0efNm6tChA9nY2Ahl+TVx4kRydXUlQ0NDGjZsGN29e5fS0tJo0aJF5ObmRm5ubjRixAiqUaMGValShVJTU8nf35/atm1bGE/3m8CJAxNNixYtqESJElS6dGny9vampUuXUmJiIjk7O9OoUaNUHk9YWBiVKlWKfvvtN5LL5RQREUHr168nAwMD+umnn4T1fH19qVq1avT+/XuVx5iTd+/e0apVq6h8+fI0ePBgoXzv3r3UunVrqlu3riTGC8TGxtKIESOoTJky1KBBA6E8LS1N+MLO/H9SUhL5+/tT+/btVRLb503YkydPpuLFi9PKlStp9OjR1LZtW7KxsREGHMbGxpKDgwPZ2NgUSQuOXC5XiOmvv/6iChUq0L59+2jXrl1UqVIlatasmTAFNyQkhMaMGUM9evSgkSNHCi0kvXv3Jj8/P0pOTpZkiwNRRkLUpk0bhbKTJ0+Svb29MEg2PDyc5s2bl+cuwc/r8c8//yQbGxvauXMnrV69mipUqEDe3t50/fp1ksvldOnSJfL396cOHTrQgAED6NOnT0RE1L17dxo4cCClpqZKth5ViRMHVuQy32h3796lCxcuCB++CQkJtHbtWlq3bh3FxsYKvz59fX1p5syZKn+Dnj17lhwdHen169dCWVJSEq1Zs4ZsbGwUfrlHRUWpNLbPPXnyROHv9+/f08qVK6l8+fI0ZMgQoXzHjh3UqVMnUWZ95HT+njx5QgEBAaSvry9cr4NI8Ys78xf7xYsXqWzZsvT06dMij/XzGKtUqUIHDx4Uyu7du0dDhgwhJycnunbtGhFlJA9F0Xyd2eWQ6fTp0zR+/HhatWqVUHb//n2qW7cuNW3aNMfrDcTHx9Po0aPJzMwsW5O/VGS+PoYOHUotWrQgoowWtMzXwpo1a8jExER4r+X18+Dz83Lx4kUaN24cbdiwQSi7cuUKVatWjdq3b09XrlzJto9Pnz7RmDFjyNzcXKGL6HvHiQMrUln7MG1tbalq1apUoUIFqlSpUrY36vv372nq1KlkZmZGDx48UHmswcHBZGRklG361aNHj6hUqVL0119/qTymnDx8+JAcHByyDRCLiYmhBQsWkKmpKU2aNEkoT0hIUHWIColAfHw8paamCh/kT58+pYkTJ5KjoyMtXLgwx22IiCZMmED29vbCwL+i0KJFC1q0aJFC2X///UcGBgZ04MABhfIbN25QtWrV6M8//8y2n8JKHvr27Ut//PEHEWXUx/Pnz8nU1JRkMlm2KaqhoaFUt25datWqlcLUy+fPn9PEiROpWrVqKhlQXFC///47aWlpCd0+mXV55MgRcnV1pbdv3+Z5nwMGDBCmbKanp9ODBw+oWLFipKGhQT///LPCulevXqXq1atT586d6dixY0L5w4cPaezYsVSpUqUCd498azhxYEXu8uXLZGpqKnwghoWFkUwmo1WrVgmJxcmTJ8nb25tsbW1V8ibNPO6NGzcoODiYkpKSKDo6mjw9Palfv34Kszw+fvxI7u7uOX5hiOHFixfCB9rnH4LPnz+ncuXKkUwmE7p7VN1ykzUBWLJkCbVs2ZIaNWpEI0eOpMTERCLKSMYmTpxIzs7O2b64M/Xr14+CgoKKLM7ExETatWtXtgGYr169ovr169Ps2bOzJV01a9ZU6LYqTOnp6TR37lxhmmxmXP/88w85OTlRgwYN6J9//lHY5sGDB2Rvb0+jR49WKA8NDaVXr14VSZz5lfk6DA0NpQsXLtCRI0eExzp37kwWFhYK7/2xY8dS9erV89wl+OHDB5o0aZJQj5mvx2PHjpG1tTW1bNmSbt26pbDN1atXycrKSiHhJiK6efMmRUZG5un43wNOHFiR++2334Rpav/99x/Z2dnRwIEDFdZ58eIFrVq1SiXTGjM/wPbu3UsWFhY0f/584cNhx44d5OTkRL1796YDBw7Qw4cPady4cWRhYaEwCltsT58+pcmTJ5OTk5NC8vD+/Xvq1asXrVq1Klt3hipkTVImTpxIFhYWtHTpUlq4cCG5urpSo0aNhC/jR48e0aRJk6h48eK0bdu2HPehKosWLVJ4TY4ePZrKlClDO3bsEJKd+Ph4ql27dpEM2Pz8Oa9du5amTJkizN65fPkylS9fnrp27ZotmXr27JnwK12q12v4fPaEi4sLubm5kaOjIz148IDCwsLIx8eHtLW1qW7dulS/fn0yNTXNc4vJ5/W4YcMGmj9/vpCEHTx4kKytral///7ZZmplDpIkkm49SgUnDqzQZb55M5seR40aRZ07d6aEhASytramgQMHCuusW7eOFixYoPIYjx8/TgYGBrR27dpsv2gOHz5MzZo1I319fXJycqLy5cuL1lSZWU+3b9+mgwcP0v79+4Uv3qdPn9KUKVPIwcGBxowZQ/fv36fx48dTgwYN6M2bNyqNM+s8e6KMLwgXFxfhF3LmfQcsLS2pRo0awnN48OABrV69WuXT3LJ+MSQnJ9PixYvJ2NhYoTWhV69eVLZsWerSpQuNGTOGPD09qVKlSkUyy+fzL7wBAwZQ1apVacGCBULycPHiRSF5uH79erZ9SH2q4N9//01GRka0bt06ksvl9O+//5JMJqPVq1cL62zZsoVmzJhB8+bNo//++y/Px8h6XtPT06lz587k7u5OK1euFJKH/fv355o8EEm/HqWAEwdWJA4fPkzm5uYUFBREly5doh9++IFMTEyEX3WZb/ARI0ZQjx49hF91qpCWlkb+/v7CtQ8+fvxI9+7do4kTJ9KKFSuE1ofQ0FC6ceOGwmBJVcraMmJra0tOTk5UrVo1cnV1FWJ88eIFLVu2jMzMzMjGxoasra1VnuR07NiR+vfvr5Cs7NmzR+iTP3ToEJmbm9PKlSvp4MGDZGBgQI0aNco2nVVVH9hZv1wyrxmSmJhIv//+O5mbmytcGXLZsmXUr18/atasGQ0ZMkRo/i7MWDMHWxJlTAHdt2+fMJXS3d2d5s2bJyQPf//9Nzk4OFDTpk0lPVgvczBu1oTot99+owEDBhBRxgBUGxsbhVlABZW1+2HhwoV09uxZSkhIID8/P/rhhx9o+fLlCsmDra0tderUSZSWOXXHiQMrNFnvPdGnTx9hFPiLFy+oW7du5OjoSFu2bCGijGvRT548mSwsLFQy6jtrK0hERAR5e3uTn58fXbt2jQYOHEhNmzYlW1tbql27NvXo0SPbL2ixnDlzhkxNTYX7EZw5c4ZkMhmVL19e6NZJTU2lV69e0dWrV0Xp1w4MDCQNDQ0aO3YsRUdHC+Xh4eGUkJBAHh4eNHv2bCIievv2LVWqVIk0NDSod+/eRKTaromsScOMGTOoTZs2QqvI+/fvadWqVWRmZqaQPMjlcoVWhsJscXjx4gVpamrSjz/+SGPGjCEjIyO6c+cOEWUkJ0OGDMmWPJw+fZq6dOki2eb0Q4cOkUwmy3aZ5sGDB1OXLl0oOjo6W8vj1q1bhddIfjx69IhkMhnNmDGDxo4dSyYmJsI4pfj4eOrVq1e25GH79u3UoUMHydajlHHiwArVv//+Sz4+PlSzZk0KDg4Wym/dukXt2rUjOzs7srGxoTp16hTKxVzy4sCBAySTyejevXu0efNmKlGiBBUvXpx8fHxo586dRJRx46rGjRurLKYviY+PpxEjRgjTFl++fEnlypWjnj17koeHB9nY2Ig+7iLzg/+vv/4imUxGY8aMUUhe7t27R1ZWVsIlriMiIqhr16509uxZUT+wJ0yYQKVLl6YdO3YoxBsfHy8kD58POCQq/CRHLpfT2bNnSVtbm4yMjIQrImZePyAtLY2GDh1KtWrVovnz52ebYSLFL73o6Gjq1asXGRkZKdzq/ejRo9S4cWMqUaIE9e/fn4j+d4ns4cOHU//+/RVuxJYXaWlptHv3btLW1iZjY2N6/PgxEf1vgGlm8lC3bl1auXKlUL+ZpFiPUsaJAyuwrKOlt2/fTm5ubqSjo0ObNm1SWC88PJyuXLlC8+bNo0OHDgnXfleF2NhYWrx4scII/gcPHggDzTI/ODIv9pPfD7DCduTIEQoKCqL3799TjRo1hO6VvXv3kkwmIzMzM5Vf5yDT5x+227dvF5KHzJaHd+/ekZubG3l7e9OFCxeoadOm1Lx5c2FbMfqTz507R2XLlhWSmdTUVIqMjKSLFy9SREQEERGtXr2aZDIZrVixosjjOXv2LGloaJCenp7CNTgyv/TS0tJo2LBhZG1tLdzBUeoXIUpISKAff/yR9PX1hWnXz549o2bNmpGNjY1w862YmBiaNGkSWVhYFLjrJbOlQ1NTk2bOnCmUZ3YvxcfHk5+fH9nZ2QlTq6Vej1LFiQMrFJm/5h88eCCMafDy8srx4jSqduvWLdLR0SFnZ+dcbzV869YtCggIIGNjY7p9+7aKI8yQ+SF269Yt4e6WmY4fP0516tQR+mMvXLhA7dq1oy5duuTruv2FKTQ0VPiS27Fjh5A8ZI552Lp1K1WuXJns7OyoYcOG2abJqdrx48epRo0aFBkZScHBwTRx4kSqUKEClS1blry8vOjhw4cUHx9P+/btK5LEJqd9vnz5kk6cOEEGBgbCOICs5HI5rVy5UvID9zLP6aVLl2j9+vWkpaVFxYsXF17PoaGh9MMPP1ClSpWoXLly5OXlRWXLls1Xy2NOr59nz57Rn3/+SVpaWtmmVhJlTNWcN2+e5OtR6jhxYAWW+Ws+84ZVRBkjwD08PMjb25vOnDkjlKsyw8/8cIiKiqJ+/fqRTCajNWvWKDxGlNGc3qRJE3JzcxPtToJZp6vZ2NjQrFmzFFoS1qxZQzo6OkJLyOTJk6lnz56UlJSk8lizfmBv3bqV3NzcaPfu3UJCkJk8/PTTT8Kg14SEBLp//76wraruP5LTl0twcDBpa2tTo0aNyNjYmPr27Uvbtm2jo0ePko2NTbZkt7Bi/fzL6tKlS3T06FGFCxzt3buXDAwMaNCgQULZoEGD6NChQ7nuR2r27t1LJiYmNHnyZBo4cCDVrFmT9PX1hc+BFy9e0KlTp2jWrFm0b9++fHW3ZT2vjx49opCQEIXLamcmLVkvkjZixAiFhFzq9ShlnDiwAvnSr/nz58+Th4dHtiuyFaXQ0FCaNGkSPXv2TOHDJTIykvz8/EhfX1+4dHTWJObGjRvCCHuxHD9+nIoVK0arVq3KlhC8f/+e3NzcqHjx4tSoUSMyMDDIdhEbVchap/v27aM5c+aQpqYmubu704EDB7IlD5+Pefh8H6qK9cmTJ/T06VNh6u3169dp3rx5tG/fPmHQYVJSElWrVk3hS7qw9O7dm3bt2iV8WY0dO5ZMTU2pVKlSwnUsMpOszOTBw8OD6tatS/b29iq90VtefP7lGxcXRzVr1qTJkycLZY8fPyZfX1/S19fP1pKWH1nft5MmTSIHBwdhVtGUKVOEGR3r168nTU1NatOmDXl4eJCjo6Nk61HdcOLA8uVLv+azvjkvXrxIrq6u1LNnzyIfN5CSkkI1a9YkmUxGDg4ONHbsWGHQI1HGlLtu3bpRsWLF6NKlSwrPQ0xyuZw+fvxInTp1ogkTJhBRRn/snTt3aPbs2TRv3jwiyvilFhAQQFOmTBF9Kl5AQACVKFGCVq5cSfPnzydHR0dyc3Oj/fv3C8nDzp07SSaT0cqVK1UeX9YvlylTppCTkxNZW1uTpaUlrVmzRmH676dPn+j9+/fUokULql27dpG8Jry8vKhEiRJ06NAhOnPmDFWpUoXOnz9PkZGRNHjwYDI3N6fff/9diOvGjRvk5+dHY8eOLZIpoIVhzJgxNGbMGIWyN2/ekI2NDa1bt04oS09Pp0ePHpGbmxtZWFgoDJgsiEWLFlGJEiXo4MGDdP/+fZo8eTLVqVOH+vXrJ0xXPnPmDHXq1ImGDRsm2XpUR5w4MKXl5dd81jfnpUuXVDaAb+HChbRkyRI6efIkTZ8+nYoXL06+vr60evVqksvlFBsbS/379ydjY2NhuphUdO/endq1a0ehoaE0cOBAaty4MVWqVInMzc2pW7duwnpijA3IevfK//77j8qWLSsMcCPK+KVZq1YtcnFxoQMHDghjHk6fPq3yX3lZk4b58+eTubk57d+/n86ePSuMY5k5c6ZwY7VZs2ZR3bp1qXbt2oX+5ZL1XPn4+FDp0qVpzpw5FBAQoLDeyJEjheQhp0ssS+mXcuZzOn78uDAeKGudd+zYkdq0aZPtct2+vr6kpaVFlpaWBfoRIZfLKSkpiZo3b55tCufq1avJ1dWVNm7cKJQV1VTa7xknDkwp+fk1L8ab9Ny5c2RsbCzMloiMjKQZM2aQnp4e1alTh9auXUt///039e7dm6ysrEQZI0D0vw/ae/fuCbe8XrVqFdWvX580NDSoc+fOtHPnTkpOTqbly5dTw4YNs901UVWyfvmlpKRQeHg4lStXjo4ePUpE/5s6GBsbSxYWFlS/fn3av3+/yj+ws15pMD09nZKSkqhRo0ZCi02m5cuXk56enhD/v//+S7NnzxZiLMxY5XK5QhLSuXNnkslk1KZNm2xTAkeNGkWlSpWixYsXS+Y6Irl5/vy5MCvl2LFjNHToUOH1uX79eqpVqxbNmDFDIUEYPHgw7dixQ+FaH/mVnp5OjRs3prFjxxKR4jnr3LkzeXh4EJFiQsMzKAoPJw5Maerya37s2LHk6+srJAVdu3YlZ2dn6t27N3l6epK2tjYFBASINqbh8zuGLly4kN6+fUtpaWn0+PFjhdt3ExH9+OOP1KFDh2w3Y1JlrEQZl0Hu378/RUdHk52dncJFklJSUig9PZ28vLzIysqK6tatSw8fPsy2j6IyZMgQ8vT0FJIwooxExtnZWbi3RNYvah8fH2ratGm2loWiasbOeqMkPz8/0tPToz179mQ7p/7+/tSqVSvJfsmlp6fTx48fqXr16jRixAgi+t/U4My/5XI5TZw4kWrWrElNmjShX375hXr37k0lSpQQXhN5PWZOevfuTY6OjsIYlcw6mzdvHrVs2ZKvzVCEOHFgSlOXX/N//fUX1alTh9LT06lfv35UqlQp4Zr0oaGhtHLlyhyvUa9Kx44dIwMDA/r1119z/HWZ2SUwevRoMjU1FWWKaNYvrydPnpCbm5swMn7Pnj2kq6srXJwqc31/f3+6fPkyWVlZKVyToKhdvnyZHB0dqXPnzgrJQ8+ePcnZ2Vn4csn8oh45ciR17NixyOLJ+qWVeZO3rFMOO3XqRGZmZnTw4EGhe+TzbaWaPBAR/fzzz2RmZia08hw+fJh0dXUVZoNs27aNfH19qUqVKtSkSZN83eI7az3ev3+fHj58KEw/TkxMJEdHR6pfvz5FRERQfHw8paSkUMOGDcnPz69Az499GScOLE+k/ms+U4MGDUhDQ4PKlCkj2hTLnMjlcvrw4QO1adNGGAiZkJBA//33Hy1atIiWLVtGRERBQUHk7+8v6hTRTIsWLaKuXbvSgAEDhF/kCQkJtHz5ctLW1qa2bdvS0KFDqV69euTs7ExEGbfEbt26tUriy/xyCQoKInt7e+rUqZNw0aGbN2/SDz/8QE2bNhXujZHZMpJ5Ma2iiocoY5Bj3759ydDQkAYOHChcTpooYyyAubk5HTp0KFvLg1R/LWe92Fu1atUUpmAfOnQoW/JAlDHQ9/NumbwciyhjMG7FihWpZMmSZGdnJ7x37t27RxUrViRra2uqUqUKubu7k6urq5CMSTn5UmecOLA8kfqv+cwPiiNHjpCjo6MwgE+MD5DPP/w/HyjXr18/unPnDg0ZMoQaN25MdnZ2VLZsWerbty8RZbTwvHz5UqUxfx5nQkICjRkzhgwNDcnT01NhvdTUVLp06RK1bduW2rdvT35+fsIHdps2bVTa4pAZ87Vr18je3p46duwoXPL80KFDVKtWLTI3N6emTZtStWrVyMXFRegXL6zXxuf7GT16NFWoUIFGjhxJnTp1Ig0NDerTp4/CNNouXbqQTCYTxgVJUW7106tXL6pYsaJCWWbyMHz48EKbRbVw4UIyMzOjU6dO0fHjx2nNmjVUrFgxhdug//bbb7Rw4UJavnx5kYxVYYo4cWB5JtVf81m9evWK7O3tacqUKaLGkXUmStYP4GnTptEPP/xAmpqa1KVLF9q+fTvFx8fT9OnTqU2bNiJG/D+Zg92eP39O06dPJ5lMRr/++qvweE5N6h8+fKBx48YVyiWEvya3X+VXr14le3t78vb2Fl6fr1+/pvnz51NAQADNnz+/yL9czp8/T+bm5sINtIgykm4zMzPy8/NTaHmYNGmSJKcIZp7Xz+9cm9k68vTpU7KyshLGkGQ6cuQIyWQyGjduXIFjSElJoQ4dOmSbPXHy5EnS1tamxYsX57idFOvzW8KJA1OalH7NK2PLli1kYGCg0OetSjnNRNmxY4fweFhYmHCVwswvwQEDBlCXLl0UroInhrVr15KdnZ1w2ejIyEgKCAggAwMD+v3334X1UlNThTjDwsJo6tSpZGdnl6/+7LzImjQcO3aMNm/eTKdPnxZugX7lyhWyt7enDh06CGNyPldYXy6DBg1SmJpKlJE4WFtb04MHD0gulwt1lHk/jyFDhmRLuqX4Czk8PJwsLCxoyJAhtHfvXoXH3r9/Tx06dKBOnToJZZnP8/jx44WSOH78+JEcHR1p1KhRQlnmeRsyZAh5e3vTp0+fOFFQMU4cWJ5J5df810RERJCnp6eo4y1ymonSvXt3WrNmjUJi8PTpUxozZgyZmpoq/BoVy4MHD8jJyYlq1qwpJA8vX76kyZMnk7GxsXCb76xSUlLo1q1bRd69krXefvrpJypRogSVLVuWnJycyNHRUegqu3r1qjBgsrAuOpRTHN27d8/22N9//03FihWjs2fPEtH/ZnTExcVRuXLlqFSpUjRq1ChKSEiQZOK9ZMkSun37Nn38+JEWLFhA1atXJysrK2rUqBHt3btXSNDOnTtHmpqahXK1zdxakGbMmEHVq1cXpn9mCggIoEaNGkmy/r51nDiwfBH717yyxJrZkSm3mSj6+vpUu3ZtWrt2LS1ZsoTGjh1LLi4uRf5LPSeff2BnfhA/evSIKlWqRNWrV1dIHqZOnUoymYz279+v8lizfklcuHCBatWqRf/88w/FxMTQpUuXqF27dmRqakoPHjyg/2vv3uNqzPM4gH+e0013MVHqiOpUatuUMGZUzJaiiMK6rAnlZcTUhEW8sm5Ns64vMZRcyqwhO9JrJ2yDl0tlxrTIuqaLVGRccgudLue7f/TqGUfMlqFzyvf9l57nOc/zO6ej5/P8rkQNfR6MjIxeueDR25CcnExSqZSIiFJSUpSackJCQsjExETpyfvu3bs0Y8YM2rhxI0kkklabir0lnjx5Qh4eHmRiYkKXL18moobmquPHj5Onpyc5ODiQjY0NpaSk0NmzZ2n69Ok0ZcoUevr06RvdxF9+zdmzZykrK0vsL5OTk0NeXl40ceJEysnJIaKGobY+Pj7i8tysdXFwYG9EHZ7m24rXjUQJCQkhHx8f0tbWpunTp4tPcaryYjNK4x/zgoICcnJyInd3dzE8lJaWUmJiokqr1vfs2UMTJkxo8rRfUlJCfn5+5OfnJw5zvXr16jupyk5ISCAdHR3avXs3ERF5e3vThx9+KE63fOvWLQoICCB9fX1av349JSUlkY+PD3l4eBARkYuLC82ePfutl+ttKC8vpxEjRlCnTp2adHY+fPgwhYeHU5cuXah///7UqVMnMjMze6OapvDwcKVF8ObOnUtdunQhExMTsrW1pczMTCJqaB719vamrl27kru7O/Xu3ZucnZ159ISKcHBgb0zVT/NtxW+NRLl8+TJt3LhRJSNR5HK52PO9rKyMtLS0aMiQIeL+xj/G586do44dO5Kfn1+TBatae5XLxpkYx44dS0ZGRvSHP/xBaTpsooYZOGUyWZMg9jbDw5YtW0hbW1upb8ODBw/I39+f/Pz8xCmPHz9+TPPmzSOZTEbOzs7k6+srdi7s27evStbxaK7y8nIKCAigDz74gC5dutRkf3Z2Nm3YsIGkUikJgvBG08pbW1uTjY0N5eTk0IEDB8jR0ZEyMzPpypUrFBAQQFKpVFw8r7i4mNLT02nhwoW0efNmHj2hQhwcGGsF6jYS5bvvvqOgoCBydXUVJ3E6efIkWVpa0tChQ5WOvX//PvXr148EQVD5xDqNVdVyuZyioqLIzMyMlixZIs7RQNTQPGRtba00BfXbdOzYMRIEgZYuXaq0PSoqisLDw2nUqFH08ccfK62XUFFRoTRl+MKFC0kqlVJhYeE7KePv8eLTe1lZWZPw8PKN+vHjx7+rX4unpyc5OTnRypUrKTY2VmlfcHCwGB5e9aDCnSJVg4MDY++QOo5ESUhIICMjI4qKiqIvvviCJBKJWL2elZVF5ubm5OvrKx7//Plzmjp1Kp0/f16lf6iPHDlCnTt3Fms95HI5TZ8+nfr06UORkZFUUlJCFy9eFJsD3tUkSteuXSMPDw8aMWKE2HclKCiIZDIZPX36lO7evUvBwcHk4eFBW7duVXrtf//7X5o5cyZ16dJFaSZJdfC672RZWRn5+/srhYfG78Gbfh8yMzNpxYoVYrgbMGAACYJAkyZNanLs6NGjydramnbu3PlGE0mxt4+DA2OtQF1GoiQlJZGWlpZSFfu4ceNo/fr1dPfuXaqrq6OTJ0+SnZ0dubi40OLFi8nT01NsaiFqvae8l29khw8fJhMTE6VFkqqrq2nGjBmkr69PpqamNHLkSBo/frz4dPouw4Ofnx/5+/vTwIEDyc3NTamqvqKigsaMGUMODg6UkZEhbr99+zalp6dTUVHROynXm2r8rE+cOEHz58+nWbNmKS1id/PmTTE8NHaYfNPwu337drKwsKAZM2aINUhERH/605+oS5cudOzYsSbfscGDBysN+2SqxcGBsVai6pEor6tid3FxIWdnZzI0NCRPT0/atm0bFRcXk7+/P3l7e9OoUaPETmiqnAq5traWnJyc6MiRI0T06xBHuVxOn3/+OTk5OdGKFSvECYve9dPptWvXyNvbm4yNjWnv3r3i9sbP6ubNm6+c3EldO/KlpaVR586dafjw4TRlyhQSBIH+/ve/i30ybt68SYGBgSQIgjhqpaV2795Nenp6lJqaKjYvvfj5DBw4kKysrCgrK+s3Z15lqsXBgbFWouqRKK+rYre1taXU1FQ6dOgQOTk5Ua9evcTmgBfblVurE9q+ffvEibGWLFlCU6dOpQULFlBKSgqZmppSYmJik9dUV1dTaGgo9evXj9atW0dPnjxplbIWFhaSr68vDR06VGlV05cXrlL3tvjc3FyysLAQP9uKigoyMDAgQRBo7ty54u++tLSU/vznP4sLTbXEnTt3aNCgQUpDVokahn9mZ2eLYWTo0KHUo0cPys7O5vCgpjg4MNaKVD0S5cUq9o8//rhJFfuZM2dIEIQmMyG21lPy5s2bSVtbm44fP041NTUUGxtLYWFh5OLiQkOGDCFBEEgQBAoODqbg4GDasmULbdq0iYh+bbaws7NrcnN6lxo/Uz8/P7Vec+J16uvr6R//+ActWrSIiBrCgZWVFc2cOZO2b99OgiBQbGysWPPwpiHozp075OjoqPTd2rRpE40ePZoEQSBTU1MKDAwkIiIfHx/S19dXyaqw7P/j4MDYe+ZVVez19fWkUCjozJkz5OjoqJIbYEJCAmlqajaZ2riRXC6niIgI+uijjyg6OprGjBlDAwcOpP79+4s3terqaoqMjKTi4uLWLDpdu3aN/P39yd3dXWkRK3X2Yhi8efMm5ebmklwuJx8fH5o6dSrV1dXR7du3ycLCggRB+N39c+7cuUOWlpYUFhZGR48epeDgYHJ2dqYZM2bQDz/8QP/85z9JKpXS119/TUREYWFhal9T877i4MDYe+jFKvaTJ0+K2wMCAmjQoEGtXiX8qnkRGre/WC2+YsUK8vT0FH+uqakRb4AvL03d2i5fvkyzZ89W++r0lxevejlAuLq6ihMvPXjwgKZNm0Y7d+58K2tPHDlyhIyNjcna2ppcXFzo6NGjdO/ePSIiqqyspN69e1N0dLTSazg8qB8ODoy9pxqr2IcNG0ZZWVkUFBREdnZ2rd4R8nWdNgMCAqhfv35UWVkpbjtz5gzZ2NjQrVu3lMqnbh0O1T08ZGRk0NChQ2nkyJGUnJwsdlS8evUqSSQS2rBhA92+fZsWLVpEzs7OSvNk/F537tx5ZY1QZWUleXh4iP0s1O13yn4lAWPsvSSTyRAfHw9BEPDJJ5/g0qVLuHjxIrS0tFBXVweJpHX+PFhYWGDgwIE4c+YM/vOf/wAARo8ejdLSUqSmpsLExAREBADo1KkTrl+/jvLycqXyCYLQKmVtrtb67N7E6dOnMW7cODg5OaGyshIJCQmIjo7G/fv3YW9vj9jYWERERMDDwwMJCQlISUmBkZHRW7u+qakpevbsqbTt7t27mDRpEmpqahAaGgpA/X6n7FcCNf6PZIy9l65evYpNmzZh7dq10NTURF1dHTQ1NVu1DAUFBYiIiICGhgYePXqEp0+fIi0tDT169AARQRAEKBQKpKamIj8/HzExMdDQ0GjVMrZljZ8hAKSlpSEvLw/Lli0DAKxcuRLp6elwdnbGV199BRMTE/z444949OgRnJycIJVK31m57t27h61btyI7Oxt37txBTk4OtLS0UF9fz79fNcbBgTEmUkVoaFRQUIDw8HDk5uYiKSkJY8aMgUKhEJ/e/f39UVVVhWPHjkEikfDNpZkaQ0Nubi5u3bqF06dPw9DQENHR0QCA+vp6rF27FmlpaXBzc8OSJUtgamraKmXLy8tDTEwMbGxssHr1apUFV9YyHBwYY2qjqKgIM2fOhEQiwYIFC+Dp6QkAGDZsGAoKCnD58mVoaWkpBQr2/+3btw8hISHo2LEjKisrYW9vj5ycHOjp6QEAFAoF1q1bh23btsHX1xdr1qyBIAit0lzw8OFDGBsbQxAEDoNtBAcHxphaaWy2kEgkWLhwIdauXYuLFy8q9b/gJ9L/r7Gm4enTp4iMjMTAgQMxbNgw7N+/H4mJibCyssLOnTthaGgIoCE8fP311xg+fDh69OihsvIy9cfBgTGmdgoKChAVFYUffvgB1tbWuHDhAoeGN5Cbm4vJkyfDysoK69evh0wmQ319PXbt2oVNmzbBzMwM33zzjRgeGGsOrutjjKkdmUyG1atX47PPPuOahhZqfBY8e/YsiouLYWxsjKysLOjr6wMANDQ0MGHCBMycORP379/HiBEjUFVVpcoiszaGgwNjTC05ODggPj6eO8y1kCAIOHDgAIKDg2FkZISlS5fC0tISgYGBqK2tBQBoampi/Pjx+PTTT6GlpYWHDx+qttCsTeGmCsYYawca+wj88ssvmDt3Lvr27YuIiAgoFAocO3YMc+bMga6uLo4fPw4dHR0ADaNonj179lbnaWDtH9c4MMZYOyAIAnJycjBlyhQUFBSgX79+ABomo/Ly8sLq1atRXV0NHx8fyOVyAA01DxwaWEtxcGCMsXbCzMwM169fx88//4xz586J2zU1NTF48GCsWbMGpaWlGDFihApLydo6bqpgjLF25MaNGxg1ahT09PSwbNkyfPLJJ+K++vp6ZGdnQyqVwtraWoWlZG0ZBwfGGGuDGvs05Ofno6ysDB07doSZmRksLS1RUFCA4OBgmJubIzo6GoMGDVJ1cVk7wsGBMcbamMbQsG/fPkRGRkJLSwtEhA4dOmDLli3w9PTEtWvXMHr0aEilUkRGRmLIkCGqLjZrJ7iPA2OMqTmFQiH+u66uDoIg4Oeff8aUKVMQExOD7OxspKSkoG/fvvD19UVWVhbs7OyQlpaGCxcuIDExEc+ePVPhO2DtCQ+MZowxNSeRSHDjxg10794dmpqaqK+vx4ULF+Du7o5p06ZBIpHAwsIC9vb2UCgUiIyMxMGDB2Fra4uTJ09CoVCI61Iw9ntxjQNjjKk5uVyOcePGwdraGkQEDQ0NPH78GHl5eXj8+DGAhuYLMzMzTJgwAffu3cODBw8AAD169OCOkOyt4uDAGGNqTltbG6tWrYKBgQHc3NxARAgMDIS5uTl27NiBhw8figtEyWQyaGlp4cmTJyouNWuvODgwxpiaebFPA9AwudNHH32EpKQkPH/+HP3794e1tTVGjRqFHTt2ICkpCb/88guqqqqwfft2SCQSlaxwyd4PPKqCMcbUiEKhgEQiwe3bt1FSUoIPP/xQ3FdbW4tz585h3LhxkEqlOHHiBBYvXoz9+/ejsLAQvXv3RlFRETIzM+Hq6qrCd8HaMw4OjDGmZsrKyuDq6orKykp4eXlhwIAB8Pb2hru7O4yMjJCbm4vQ0FAYGRkhOzsbt2/fxsGDB2FiYgI3NzdYWVmp+i2wdoyDA2OMqZkbN25g5MiReP78OQwNDeHk5ITU1FQ4ODjA2dkZAQEBEAQB0dHRsLa2RmZmptjHgbF3jYMDY4ypocLCQsybNw8KhQLR0dEwNzfHqVOnsHHjRtTW1uLixYuwsbHBxYsXERgYiP3794sTQzH2LnFwYIwxNZWfn4/IyEgoFArExsaib9++AICHDx/i+++/x9WrV3Ho0CFs27aN+zSwVsPBgTHG1FhBQQE+//xzAEB0dDS8vLyU9tfV1UFTk+fyY62Hh2Myxpgak8lk2LBhAwRBQFxcHE6dOqW0n0MDa20cHBhjTM3JZDLEx8dDS0sLc+bMwU8//aTqIrH3GAcHxhhrA2QyGVatWgVLS0t069ZN1cVh7zHu48AYY21ITU0NtLW1VV0M9h7j4MAYY4yxZuOmCsYYY4w1GwcHxhhjjDUbBwfGGGOMNRsHB8YYY4w1GwcHxhhjjDUbBwfGGGOMNRsHB8bYG5s8eTJGjhwp/jxo0CB88cUXrV6O48ePQxAEPHz48LXHCIKA9PT0Zp9zyZIl6N279+8qV0lJCQRBQF5e3u86D2PqhIMDY+3M5MmTIQgCBEGAtrY2bG1tsWzZMtTV1b3za6elpWH58uXNOrY5N3vGmPrh1VEYa4f8/PywY8cOyOVyHDx4EDNnzoSWlhaio6ObHPs2ZyLs1KnTWzkPY0x9cY0DY+2Qjo4OzMzMYGVlhRkzZsDb2xv/+te/APzavBAbG4tu3brB3t4eAFBWVoaxY8eiY8eO6NSpEwIDA1FSUiKes76+HrNnz0bHjh3RuXNnzJs3Dy9PPPtyU4VcLsf8+fMhlUqho6MDW1tbbNu2DSUlJRg8eDAAwMTEBIIgYPLkyQAAhUKBuLg49OzZE7q6unBxccF3332ndJ2DBw/Czs4Ourq6GDx4sFI5m2v+/Pmws7ODnp4erK2tERMTg9ra2ibHJSYmQiqVQk9PD2PHjsWjR4+U9m/duhW9evVChw4d4ODggE2bNrW4LIy1JRwcGHsP6OrqoqamRvz56NGjyM/Px+HDh5GRkYHa2lr4+vrC0NAQWVlZyMnJgYGBAfz8/MTXrVmzBsnJydi+fTuys7NRWVmJ/fv3/+Z1P/30U+zevRvx8fG4cuUKEhMTYWBgAKlUin379gEA8vPzUVFRgfXr1wMA4uLisHPnTiQkJODSpUuIiorCX/7yF5w4cQJAQ8AJCgrC8OHDkZeXh7CwMCxYsKDFn4mhoSGSk5Nx+fJlrF+/HklJSVi3bp3SMYWFhdi7dy++//57/Pvf/8a5c+cQHh4u7t+1axcWL16M2NhYXLlyBV9++SViYmKQkpLS4vIw1mYQY6xdCQkJocDAQCIiUigUdPjwYdLR0aG5c+eK+7t27UpyuVx8zTfffEP29vakUCjEbXK5nHR1dSkzM5OIiMzNzWnlypXi/traWrK0tBSvRUTk5eVFkZGRRESUn59PAOjw4cOvLOexY8cIAD148EDcVl1dTXp6enTq1CmlY0NDQ2n8+PFERBQdHU2Ojo5K++fPn9/kXC8DQPv373/t/lWrVlGfPn3En//2t7+RhoYGlZeXi9sOHTpEEomEKioqiIjIxsaGvv32W6XzLF++nAYMGEBERNevXycAdO7cuddel7G2hvs4MNYOZWRkwMDAALW1tVAoFJgwYQKWLFki7nd2dlbq13D+/HkUFhbC0NBQ6TzV1dUoKirCo0ePUFFRgf79+4v7NDU14e7u3qS5olFeXh40NDTg5eXV7HIXFhbi2bNn8PHxUdpeU1MDV1dXAMCVK1eUygEAAwYMaPY1GqWmpiI+Ph5FRUWoqqpCXV0djIyMlI7p3r07LCwslK6jUCiQn58PQ0NDFBUVITQ0FNOmTROPqaurg7GxcYvLw1hbwcGBsXZo8ODB2Lx5M7S1tdGtWzdoair/V9fX11f6uaqqCn369MGuXbuanMvU1PSNyqCrq9vi11RVVQEADhw4oHTDBhr6bbwtP/74IyZOnIilS5fC19cXxsbG2LNnD9asWdPisiYlJTUJMhoaGm+trIypGw4OjLVD+vr6sLW1bfbxbm5uSE1NRZcuXZo8dTcyNzfH6dOn4enpCaDhyfrMmTNwc3N75fHOzs5QKBQ4ceIEvL29m+xvrPGor68Xtzk6OkJHRwelpaWvrano1auX2NGz0U8//fT/3+QLTp06BSsrKyxatEjcduPGjSbHlZaW4tatW+jWrZt4HYlEAnt7e3Tt2hXdunVDcXExJk6c2KLrM9aWcedIxhgmTpyIDz74AIGBgcjKysL169dx/PhxREREoLy8HAAQGRmJr776Cunp6bh69SrCw8N/cw6GHj16ICQkBFOnTkV6erp4zr179wIArKysIAgCMjIycPfuXVRVVcHQ0BBz585FVFQUUlJSUFRUhLNnz2LDhg1ih8PPPvsMBQUF+Otf/4r8/Hx8++23SE5ObtH7lclkKC0txZ49e1BUVIT4+PhXdvTs0KEDQkJCcP78eWRlZSEiIgJjx46FmZkZAGDp0qWIi4tDfHw8rl27hgsXLmDHjh1Yu3Zti8rDWFvCwYExBj09PZw8eRLdu3dHUFAQevXqhdDQUFRXV4s1EHPmzMGkSZMQEhKCAQMGwNDQEKNGjfrN827evBmjR49GeHg4HBwcMG3aNDx9+hQAYGFhgaVLl2LBggXo2rUrZs2aBQBYvnw5YmJiEBcXh169esHPzw8HDhxAz549ATT0O9i3bx/S09Ph4uKChIQEfPnlly16vyNGjEBUVBRmzZqF3r1749SpU4iJiWlynK2tLYKCgjBs2DAMGTIEf/zjH5WGW4aFhWHr1q3YsWMHnJ2d4eXlheTkZLGsjLVHAr2uZxNjjDHG2Eu4xoExxhhjzcbBgTHGGGPNxsGBMcYYY83GwYExxhhjzcbBgTHGGGPNxsGBMcYYY83GwYExxhhjzcbBgTHGGGPNxsGBMcYYY83GwYExxhhjzcbBgTHGGGPNxsGBMcYYY832Pxq+Dy8dCVlHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# freeze all layers except classification head\n",
        "for name, param in model.named_parameters():\n",
        "    if 'cls_head' not in name:\n",
        "        param.requires_grad = False\n",
        "\n",
        "\n",
        "# : new optimizer for just the head\n",
        "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "\n",
        "# retrain classifier with same early stopping\n",
        "best_test_auc = 0.0\n",
        "patience, patience_counter = 3, 0\n",
        "\n",
        "\n",
        "for epoch in range(1, 11):\n",
        "    model.train()\n",
        "    train_loss, train_preds, train_targets = 0, [], []\n",
        "\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = F.cross_entropy(outputs, batch_y, weight=class_weights_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_loss += loss.item() * batch_x.size(0)\n",
        "        train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        train_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_targets, train_preds)\n",
        "    train_auc = roc_auc_score(\n",
        "        F.one_hot(torch.tensor(train_targets), num_classes=len(CLASS_NAMES)),\n",
        "        F.one_hot(torch.tensor(train_preds), num_classes=len(CLASS_NAMES)),\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "\n",
        "\n",
        "    # eval\n",
        "    model.eval()\n",
        "    test_loss, test_preds, test_targets = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            loss = F.cross_entropy(outputs, batch_y)\n",
        "            test_loss += loss.item() * batch_x.size(0)\n",
        "            test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            test_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = accuracy_score(test_targets, test_preds)\n",
        "    test_auc = roc_auc_score(\n",
        "        F.one_hot(torch.tensor(test_targets), num_classes=len(CLASS_NAMES)),\n",
        "        F.one_hot(torch.tensor(test_preds), num_classes=len(CLASS_NAMES)),\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"[FT-Classifier] Epoch {epoch:02d} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f} | Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n",
        "\n",
        "\n",
        "    if test_auc > best_test_auc:\n",
        "        best_test_auc = test_auc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), \"best_model_finetuned_head.pt\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"Early stopping triggered on classifier fine-tune.\")\n",
        "            break\n",
        "\n",
        "\n",
        "#  confusion matrix for final predictions\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "model.load_state_dict(torch.load(\"best_model_finetuned_head.pt\"))\n",
        "model.eval()\n",
        "final_preds, final_targets = [], []\n",
        "with torch.no_grad():\n",
        "    for batch_x, batch_y in test_loader:\n",
        "        batch_x = batch_x.to(device)\n",
        "        outputs = model(batch_x)\n",
        "        final_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        final_targets.extend(batch_y.numpy())\n",
        "\n",
        "\n",
        "cm = confusion_matrix(final_targets, final_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=CLASS_NAMES)\n",
        "disp.plot(xticks_rotation=45, cmap='Blues')\n",
        "plt.title(\"Confusion Matrix - Best Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 7: Grid Search & Final Model Deployment**\n",
        "(Hyperparameter tuning, retraining best model, multi-sequence final training)"
      ],
      "metadata": {
        "id": "BRoh5J5-Fmyh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrFavSiMbC0S",
        "outputId": "9b13d0e4-838d-4dfb-d8c3-6d9bd0fabe1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Config 1/15: {'sequence_length': 16, 'dropout': 0.4, 'hidden_dim': 128, 'num_layers': 2, 'lr': 0.001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 2.5454, Acc: 0.1847, AUC: 0.5301 | Test Loss: 2.5469, Acc: 0.2000, AUC: 0.5293\n",
            "[Epoch 2] Train Loss: 2.0586, Acc: 0.1289, AUC: 0.4955 | Test Loss: 2.0935, Acc: 0.0769, AUC: 0.4984\n",
            "[Epoch 3] Train Loss: 2.0585, Acc: 0.1603, AUC: 0.5177 | Test Loss: 2.1122, Acc: 0.1385, AUC: 0.5352\n",
            "\n",
            " Config 2/15: {'sequence_length': 16, 'dropout': 0.3, 'hidden_dim': 128, 'num_layers': 1, 'lr': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 1.9720, Acc: 0.2125, AUC: 0.5494 | Test Loss: 2.0269, Acc: 0.2308, AUC: 0.5517\n",
            "[Epoch 2] Train Loss: 1.7208, Acc: 0.3240, AUC: 0.6129 | Test Loss: 2.0103, Acc: 0.2462, AUC: 0.5637\n",
            "[Epoch 3] Train Loss: 1.3909, Acc: 0.4634, AUC: 0.7003 | Test Loss: 1.9795, Acc: 0.2923, AUC: 0.5706\n",
            "\n",
            " Config 3/15: {'sequence_length': 32, 'dropout': 0.3, 'hidden_dim': 256, 'num_layers': 2, 'lr': 0.001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 4.6677, Acc: 0.1220, AUC: 0.4985 | Test Loss: 2.1617, Acc: 0.1231, AUC: 0.5000\n",
            "[Epoch 2] Train Loss: 2.1293, Acc: 0.1080, AUC: 0.4956 | Test Loss: 2.0731, Acc: 0.1385, AUC: 0.5000\n",
            "[Epoch 3] Train Loss: 2.0736, Acc: 0.1533, AUC: 0.5000 | Test Loss: 2.0763, Acc: 0.1385, AUC: 0.5000\n",
            "\n",
            " Config 4/15: {'sequence_length': 32, 'dropout': 0.3, 'hidden_dim': 128, 'num_layers': 2, 'lr': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 1.8777, Acc: 0.2927, AUC: 0.5798 | Test Loss: 2.0457, Acc: 0.2154, AUC: 0.5521\n",
            "[Epoch 2] Train Loss: 1.4411, Acc: 0.4425, AUC: 0.6726 | Test Loss: 2.1539, Acc: 0.2308, AUC: 0.5619\n",
            "[Epoch 3] Train Loss: 1.2387, Acc: 0.5192, AUC: 0.7218 | Test Loss: 2.1634, Acc: 0.2923, AUC: 0.5831\n",
            "\n",
            " Config 5/15: {'sequence_length': 16, 'dropout': 0.4, 'hidden_dim': 256, 'num_layers': 1, 'lr': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 1.9656, Acc: 0.1986, AUC: 0.5376 | Test Loss: 2.2716, Acc: 0.1077, AUC: 0.4906\n",
            "[Epoch 2] Train Loss: 1.7267, Acc: 0.3171, AUC: 0.6029 | Test Loss: 2.1070, Acc: 0.2462, AUC: 0.5742\n",
            "[Epoch 3] Train Loss: 1.3768, Acc: 0.4739, AUC: 0.6917 | Test Loss: 1.9332, Acc: 0.3077, AUC: 0.6057\n",
            "\n",
            " Config 6/15: {'sequence_length': 16, 'dropout': 0.4, 'hidden_dim': 128, 'num_layers': 2, 'lr': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 2.0228, Acc: 0.1847, AUC: 0.5358 | Test Loss: 2.0949, Acc: 0.2308, AUC: 0.5663\n",
            "[Epoch 2] Train Loss: 1.6965, Acc: 0.3310, AUC: 0.6135 | Test Loss: 2.2533, Acc: 0.1692, AUC: 0.5481\n",
            "[Epoch 3] Train Loss: 1.3810, Acc: 0.4286, AUC: 0.6773 | Test Loss: 2.1622, Acc: 0.2000, AUC: 0.5206\n",
            "\n",
            " Config 7/15: {'sequence_length': 16, 'dropout': 0.3, 'hidden_dim': 128, 'num_layers': 1, 'lr': 0.001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 2.9541, Acc: 0.1324, AUC: 0.5032 | Test Loss: 2.1308, Acc: 0.1538, AUC: 0.5000\n",
            "[Epoch 2] Train Loss: 2.0663, Acc: 0.1359, AUC: 0.5032 | Test Loss: 2.6948, Acc: 0.1538, AUC: 0.5000\n",
            "[Epoch 3] Train Loss: 2.1069, Acc: 0.1080, AUC: 0.4932 | Test Loss: 2.0766, Acc: 0.1538, AUC: 0.5000\n",
            "\n",
            " Config 8/15: {'sequence_length': 16, 'dropout': 0.4, 'hidden_dim': 256, 'num_layers': 2, 'lr': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 2.0873, Acc: 0.2544, AUC: 0.5690 | Test Loss: 2.3094, Acc: 0.2462, AUC: 0.5685\n",
            "[Epoch 2] Train Loss: 1.7886, Acc: 0.2962, AUC: 0.5968 | Test Loss: 2.0748, Acc: 0.2462, AUC: 0.5693\n",
            "[Epoch 3] Train Loss: 1.3943, Acc: 0.4460, AUC: 0.6907 | Test Loss: 1.9433, Acc: 0.3231, AUC: 0.6128\n",
            "\n",
            " Config 9/15: {'sequence_length': 16, 'dropout': 0.3, 'hidden_dim': 256, 'num_layers': 1, 'lr': 0.001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 3.0263, Acc: 0.1359, AUC: 0.5055 | Test Loss: 2.2915, Acc: 0.1385, AUC: 0.5000\n",
            "[Epoch 2] Train Loss: 2.0197, Acc: 0.2160, AUC: 0.5464 | Test Loss: 2.0936, Acc: 0.1538, AUC: 0.5221\n",
            "[Epoch 3] Train Loss: 2.0923, Acc: 0.1707, AUC: 0.5188 | Test Loss: 2.1406, Acc: 0.0769, AUC: 0.5001\n",
            "\n",
            " Config 10/15: {'sequence_length': 32, 'dropout': 0.3, 'hidden_dim': 128, 'num_layers': 1, 'lr': 0.001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 3.9783, Acc: 0.1185, AUC: 0.4967 | Test Loss: 2.1797, Acc: 0.0769, AUC: 0.5000\n",
            "[Epoch 2] Train Loss: 2.0913, Acc: 0.1080, AUC: 0.4879 | Test Loss: 2.0743, Acc: 0.1538, AUC: 0.5000\n",
            "[Epoch 3] Train Loss: 2.0800, Acc: 0.1359, AUC: 0.5000 | Test Loss: 2.0765, Acc: 0.1538, AUC: 0.5000\n",
            "\n",
            " Config 11/15: {'sequence_length': 32, 'dropout': 0.4, 'hidden_dim': 128, 'num_layers': 2, 'lr': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 2.0614, Acc: 0.2056, AUC: 0.5404 | Test Loss: 1.9628, Acc: 0.2462, AUC: 0.5516\n",
            "[Epoch 2] Train Loss: 1.6601, Acc: 0.3449, AUC: 0.6190 | Test Loss: 1.8535, Acc: 0.2923, AUC: 0.5617\n",
            "[Epoch 3] Train Loss: 1.4380, Acc: 0.3868, AUC: 0.6477 | Test Loss: 2.1727, Acc: 0.2000, AUC: 0.5720\n",
            "\n",
            " Config 12/15: {'sequence_length': 16, 'dropout': 0.4, 'hidden_dim': 128, 'num_layers': 1, 'lr': 0.001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 2.5269, Acc: 0.1324, AUC: 0.5069 | Test Loss: 2.3590, Acc: 0.1385, AUC: 0.5003\n",
            "[Epoch 2] Train Loss: 2.0258, Acc: 0.1672, AUC: 0.5198 | Test Loss: 2.1642, Acc: 0.2154, AUC: 0.5328\n",
            "[Epoch 3] Train Loss: 1.9999, Acc: 0.1742, AUC: 0.5332 | Test Loss: 3.0083, Acc: 0.1231, AUC: 0.5044\n",
            "\n",
            " Config 13/15: {'sequence_length': 16, 'dropout': 0.3, 'hidden_dim': 256, 'num_layers': 2, 'lr': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 2.1072, Acc: 0.1742, AUC: 0.5281 | Test Loss: 2.0831, Acc: 0.2154, AUC: 0.5351\n",
            "[Epoch 2] Train Loss: 1.6045, Acc: 0.3659, AUC: 0.6334 | Test Loss: 2.0999, Acc: 0.2462, AUC: 0.5830\n",
            "[Epoch 3] Train Loss: 1.2294, Acc: 0.5331, AUC: 0.7336 | Test Loss: 2.0045, Acc: 0.3692, AUC: 0.6134\n",
            "\n",
            " Config 14/15: {'sequence_length': 32, 'dropout': 0.3, 'hidden_dim': 128, 'num_layers': 1, 'lr': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 2.0278, Acc: 0.2369, AUC: 0.5472 | Test Loss: 1.8661, Acc: 0.3846, AUC: 0.6089\n",
            "[Epoch 2] Train Loss: 1.6844, Acc: 0.3728, AUC: 0.6415 | Test Loss: 1.7811, Acc: 0.2000, AUC: 0.5287\n",
            "[Epoch 3] Train Loss: 1.3857, Acc: 0.4913, AUC: 0.7147 | Test Loss: 1.8675, Acc: 0.2308, AUC: 0.5580\n",
            "\n",
            " Config 15/15: {'sequence_length': 16, 'dropout': 0.4, 'hidden_dim': 128, 'num_layers': 1, 'lr': 0.0001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train Loss: 2.0369, Acc: 0.2160, AUC: 0.5506 | Test Loss: 2.0306, Acc: 0.2308, AUC: 0.5439\n",
            "[Epoch 2] Train Loss: 1.7377, Acc: 0.3240, AUC: 0.6081 | Test Loss: 2.0628, Acc: 0.2000, AUC: 0.5317\n",
            "[Epoch 3] Train Loss: 1.5074, Acc: 0.4077, AUC: 0.6613 | Test Loss: 1.8304, Acc: 0.3692, AUC: 0.6165\n",
            "\n",
            " Best Config: {'sequence_length': 16, 'dropout': 0.4, 'hidden_dim': 128, 'num_layers': 1, 'lr': 0.0001}\n",
            " Best AUC: 0.6165\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import random\n",
        "from copy import deepcopy\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "param_grid = {\n",
        "    'sequence_length': [16, 32],\n",
        "    'dropout': [0.3, 0.4],\n",
        "    'hidden_dim': [128, 256],\n",
        "    'num_layers': [1, 2],\n",
        "    'lr': [1e-3, 1e-4]\n",
        "}\n",
        "\n",
        "\n",
        "# combintions\n",
        "combinations = list(itertools.product(*param_grid.values()))\n",
        "random.shuffle(combinations)  # fir diversity\n",
        "combinations = combinations[:12]  #  12 configs\n",
        "param_names = list(param_grid.keys())\n",
        "\n",
        "\n",
        "#  logging metrics\n",
        "logs = []\n",
        "best_auc = 0\n",
        "best_config = None\n",
        "\n",
        "\n",
        "for i, combo in enumerate(combinations):\n",
        "    config = dict(zip(param_names, combo))\n",
        "    print(f\"\\n Config {i+1}/12: {config}\")\n",
        "\n",
        "\n",
        "    train_dataset.sequence_length = config['sequence_length']\n",
        "    test_dataset.sequence_length = config['sequence_length']\n",
        "    train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "\n",
        "    class GridVideoTransformer(nn.Module):\n",
        "        def __init__(self, feature_dim, num_classes, sequence_length, dropout, hidden_dim, num_layers):\n",
        "            super().__init__()\n",
        "            self.positional_encoding = nn.Parameter(torch.randn(1, sequence_length, feature_dim))\n",
        "            self.transformer = nn.TransformerEncoder(\n",
        "                nn.TransformerEncoderLayer(\n",
        "                    d_model=feature_dim,\n",
        "                    nhead=4,\n",
        "                    dim_feedforward=512,\n",
        "                    dropout=dropout,\n",
        "                    activation='gelu',\n",
        "                    batch_first=True,\n",
        "                    norm_first=True\n",
        "                ),\n",
        "                num_layers=num_layers\n",
        "            )\n",
        "            self.cls_head = nn.Sequential(\n",
        "                nn.LayerNorm([sequence_length, feature_dim]),\n",
        "                nn.Flatten(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(feature_dim * sequence_length, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_dim, num_classes)\n",
        "            )\n",
        "\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = x + self.positional_encoding[:, :x.size(1), :]\n",
        "            x = self.transformer(x)\n",
        "            return self.cls_head(x)\n",
        "\n",
        "\n",
        "    model = GridVideoTransformer(\n",
        "        feature_dim=768,\n",
        "        num_classes=len(CLASS_NAMES),\n",
        "        sequence_length=config['sequence_length'],\n",
        "        dropout=config['dropout'],\n",
        "        hidden_dim=config['hidden_dim'],\n",
        "        num_layers=config['num_layers']\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5)\n",
        "\n",
        "\n",
        "    for epoch in range(3):\n",
        "        model.train()\n",
        "        train_loss, train_preds, train_targets = 0, [], []\n",
        "\n",
        "\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_x)\n",
        "            loss = F.cross_entropy(outputs, batch_y, weight=class_weights_tensor)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            train_loss += loss.item() * batch_x.size(0)\n",
        "            train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            train_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = accuracy_score(train_targets, train_preds)\n",
        "        train_auc = roc_auc_score(\n",
        "            F.one_hot(torch.tensor(train_targets), num_classes=len(CLASS_NAMES)),\n",
        "            F.one_hot(torch.tensor(train_preds), num_classes=len(CLASS_NAMES)),\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        test_loss, test_preds, test_targets = 0, [], []\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in test_loader:\n",
        "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "                outputs = model(batch_x)\n",
        "                loss = F.cross_entropy(outputs, batch_y)\n",
        "                test_loss += loss.item() * batch_x.size(0)\n",
        "                test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "                test_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        test_acc = accuracy_score(test_targets, test_preds)\n",
        "        test_auc = roc_auc_score(\n",
        "            F.one_hot(torch.tensor(test_targets), num_classes=len(CLASS_NAMES)),\n",
        "            F.one_hot(torch.tensor(test_preds), num_classes=len(CLASS_NAMES)),\n",
        "            multi_class='ovr'\n",
        "        )\n",
        "\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f} | Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n",
        "\n",
        "\n",
        "        logs.append({\n",
        "            **config,\n",
        "            'epoch': epoch + 1,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc': train_acc,\n",
        "            'train_auc': train_auc,\n",
        "            'test_loss': test_loss,\n",
        "            'test_acc': test_acc,\n",
        "            'test_auc': test_auc\n",
        "        })\n",
        "\n",
        "\n",
        "        if test_auc > best_auc:\n",
        "            best_auc = test_auc\n",
        "            best_config = deepcopy(config)\n",
        "            torch.save(model.state_dict(), \"best_grid_model.pt\")\n",
        "\n",
        "\n",
        "# saving\n",
        "pd.DataFrame(logs).to_csv(\"grid_search_results.csv\", index=False)\n",
        "print(\"\\n Best Config:\", best_config)\n",
        "print(f\" Best AUC: {best_auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 8: Final Training with Best Configuration**\n",
        "(Trained model using best config hyperparameters from grid search: sequence_length=16, dropout=0.4, hidden_dim=128, num_layers=1, lr=0.0001)"
      ],
      "metadata": {
        "id": "S05yX6hVGZ3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#  Best Config from Grid Search\n",
        "BEST_CONFIG = {\n",
        "    'sequence_length': 16,\n",
        "    'dropout': 0.4,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 1,\n",
        "    'lr': 0.0001\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FinalVideoTransformer(nn.Module):\n",
        "    def __init__(self, feature_dim, num_classes, sequence_length, dropout, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, sequence_length, feature_dim))\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=feature_dim,\n",
        "                nhead=4,\n",
        "                dim_feedforward=512,\n",
        "                dropout=dropout,\n",
        "                activation='gelu',\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.LayerNorm([sequence_length, feature_dim]),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feature_dim * sequence_length, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, len(CLASS_NAMES))\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer(x)\n",
        "        return self.cls_head(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset.sequence_length = BEST_CONFIG['sequence_length']\n",
        "test_dataset.sequence_length = BEST_CONFIG['sequence_length']\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = FinalVideoTransformer(\n",
        "    feature_dim=768,\n",
        "    num_classes=len(CLASS_NAMES),\n",
        "    sequence_length=BEST_CONFIG['sequence_length'],\n",
        "    dropout=BEST_CONFIG['dropout'],\n",
        "    hidden_dim=BEST_CONFIG['hidden_dim'],\n",
        "    num_layers=BEST_CONFIG['num_layers']\n",
        ").to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=BEST_CONFIG['lr'], weight_decay=1e-5)\n",
        "\n",
        "\n",
        "# Training Loop\n",
        "best_auc = 0\n",
        "patience = 3\n",
        "wait = 0\n",
        "\n",
        "\n",
        "print(\"\\n training \\n\")\n",
        "\n",
        "\n",
        "for epoch in range(15):\n",
        "    model.train()\n",
        "    train_loss, train_preds, train_targets = 0, [], []\n",
        "\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = F.cross_entropy(outputs, batch_y, weight=class_weights_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        train_loss += loss.item() * batch_x.size(0)\n",
        "        train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        train_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_targets, train_preds)\n",
        "    train_auc = roc_auc_score(\n",
        "        F.one_hot(torch.tensor(train_targets), num_classes=len(CLASS_NAMES)),\n",
        "        F.one_hot(torch.tensor(train_preds), num_classes=len(CLASS_NAMES)),\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    test_loss, test_preds, test_targets = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            loss = F.cross_entropy(outputs, batch_y)\n",
        "            test_loss += loss.item() * batch_x.size(0)\n",
        "            test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            test_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = accuracy_score(test_targets, test_preds)\n",
        "    test_auc = roc_auc_score(\n",
        "        F.one_hot(torch.tensor(test_targets), num_classes=len(CLASS_NAMES)),\n",
        "        F.one_hot(torch.tensor(test_preds), num_classes=len(CLASS_NAMES)),\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "\n",
        "\n",
        "    print(f\"[Epoch {epoch+1:02d}] Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, AUC: {train_auc:.4f} | \"\n",
        "          f\"Test Loss: {test_loss:.4f}, Acc: {test_acc:.4f}, AUC: {test_auc:.4f}\")\n",
        "\n",
        "\n",
        "    #  early Stopping\n",
        "    if test_auc > best_auc:\n",
        "        best_auc = test_auc\n",
        "        wait = 0\n",
        "        torch.save(model.state_dict(), \"final_best_model.pt\")\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQk23wsWLn_6",
        "outputId": "52428eb3-f02b-4da6-e167-bf020980d19a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " training \n",
            "\n",
            "[Epoch 01] Train Loss: 2.1237, Acc: 0.1742, AUC: 0.5224 | Test Loss: 2.0002, Acc: 0.2000, AUC: 0.5241\n",
            "[Epoch 02] Train Loss: 1.7859, Acc: 0.2718, AUC: 0.5794 | Test Loss: 1.9926, Acc: 0.1846, AUC: 0.5386\n",
            "[Epoch 03] Train Loss: 1.6862, Acc: 0.3345, AUC: 0.6171 | Test Loss: 1.9808, Acc: 0.2615, AUC: 0.5731\n",
            "[Epoch 04] Train Loss: 1.4179, Acc: 0.4321, AUC: 0.6751 | Test Loss: 2.0212, Acc: 0.2308, AUC: 0.5444\n",
            "[Epoch 05] Train Loss: 1.3185, Acc: 0.4739, AUC: 0.7043 | Test Loss: 1.9310, Acc: 0.2923, AUC: 0.5736\n",
            "[Epoch 06] Train Loss: 1.1035, Acc: 0.5784, AUC: 0.7497 | Test Loss: 2.0374, Acc: 0.3077, AUC: 0.5928\n",
            "[Epoch 07] Train Loss: 0.9893, Acc: 0.5784, AUC: 0.7524 | Test Loss: 2.0204, Acc: 0.3077, AUC: 0.6054\n",
            "[Epoch 08] Train Loss: 0.9554, Acc: 0.6272, AUC: 0.7941 | Test Loss: 1.9083, Acc: 0.2923, AUC: 0.5830\n",
            "[Epoch 09] Train Loss: 0.8524, Acc: 0.6969, AUC: 0.8306 | Test Loss: 1.8690, Acc: 0.3385, AUC: 0.6051\n",
            "[Epoch 10] Train Loss: 0.6609, Acc: 0.7526, AUC: 0.8532 | Test Loss: 2.2942, Acc: 0.2154, AUC: 0.5515\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fkuYnNdLn9x",
        "outputId": "9330ff3c-5e6c-4ffc-cad8-402cf7356d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.26.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wu1q_6vDGzKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 9: Multi-Sequence Model Training**\n",
        "(Created a new dataset class to extract up to 10 sequences per video using stride=12, resize frames to 224×224, extract features with DINOv2, and prepare dataloaders for final training)"
      ],
      "metadata": {
        "id": "nFjeSiLOG1H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import torch\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "class VideoDatasetClass2(Dataset):\n",
        "    def __init__(self, base_dir, label_encoder, sequence_length=16, image_size=224, stride=12):\n",
        "        self.sequence_length = sequence_length\n",
        "        self.image_size = image_size\n",
        "        self.samples = []\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "\n",
        "        self.processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "        self.dinov2 = AutoModel.from_pretrained(\"facebook/dinov2-base\").eval().to(\"cuda\")\n",
        "\n",
        "\n",
        "        for class_name in os.listdir(base_dir):\n",
        "            class_path = os.path.join(base_dir, class_name)\n",
        "            if not os.path.isdir(class_path): continue\n",
        "\n",
        "\n",
        "            for video_folder in os.listdir(class_path):\n",
        "                video_path = os.path.join(class_path, video_folder)\n",
        "                frame_paths = sorted(glob(os.path.join(video_path, '*.png')))\n",
        "                label = self.label_encoder.transform([class_name])[0]\n",
        "\n",
        "\n",
        "                # create sequences with stride\n",
        "                sequences = []\n",
        "                for i in range(0, len(frame_paths) - sequence_length + 1, stride):\n",
        "                    clip = frame_paths[i:i + sequence_length]\n",
        "                    if len(clip) == sequence_length:\n",
        "                        sequences.append((clip, label))\n",
        "\n",
        "\n",
        "                sequences = sequences[:10]  # cap to 10 per video\n",
        "                self.samples.extend(sequences)\n",
        "\n",
        "\n",
        "        print(f\"total sequences: {len(self.samples)}\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        paths, label = self.samples[idx]\n",
        "        images = [Image.open(p).convert(\"RGB\").resize((self.image_size, self.image_size)) for p in paths]\n",
        "        inputs = self.processor(images=images, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            features = self.dinov2(**inputs).last_hidden_state.mean(dim=1)  # [sequence_length, 768]\n",
        "\n",
        "\n",
        "        return features.cpu(), torch.tensor(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "GMwD2s4qLn73",
        "outputId": "1d5deda4-7c3c-481c-bb89-8924ea458baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmarium-waseem\u001b[0m (\u001b[33mmarium-waseem-city-university-of-london\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(CLASS_NAMES)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "dtGaYD_5Ln5n",
        "outputId": "21762d19-5c88-4c73-83eb-720cbd86d16c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LabelEncoder</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.preprocessing.LabelEncoder.html\">?<span>Documentation for LabelEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LabelEncoder()</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "train_dataset = VideoDatasetClass2(\n",
        "    base_dir=\"data_trimmed_restructured/Train\",\n",
        "    label_encoder=label_encoder,\n",
        "    sequence_length=16,\n",
        "    image_size=224,\n",
        "    stride=12\n",
        ")\n",
        "\n",
        "\n",
        "test_dataset = VideoDatasetClass2(\n",
        "    base_dir=\"data_trimmed_restructured/Test\",\n",
        "    label_encoder=label_encoder,\n",
        "    sequence_length=16,\n",
        "    image_size=224,\n",
        "    stride=12\n",
        ")\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GDxvGCsLn3d",
        "outputId": "283ba494-e7d9-4b8c-df91-a86e650103d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total sequences: 2531\n",
            "total sequences: 607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stage 10: Final Model Training & Logging**\n",
        "(Defined final Transformer model with best config, applied label smoothing for stable learning, and logged training with Weights & Biases)"
      ],
      "metadata": {
        "id": "8qtWH5Z1IoVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "\n",
        "# config\n",
        "BEST_CONFIG = {\n",
        "    'sequence_length': 16,\n",
        "    'dropout': 0.4,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 1,\n",
        "    'lr': 0.0001\n",
        "}\n",
        "\n",
        "\n",
        "# model\n",
        "class FinalVideoTransformer2(nn.Module):\n",
        "    def __init__(self, feature_dim, num_classes, sequence_length, dropout, hidden_dim, num_layers):\n",
        "        super().__init__()\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, sequence_length, feature_dim))\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=feature_dim,\n",
        "                nhead=4,\n",
        "                dim_feedforward=512,\n",
        "                dropout=dropout,\n",
        "                activation='gelu',\n",
        "                batch_first=True,\n",
        "                norm_first=True\n",
        "            ),\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.cls_head = nn.Sequential(\n",
        "            nn.LayerNorm([sequence_length, feature_dim]),\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(feature_dim * sequence_length, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, len(CLASS_NAMES))\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
        "        x = self.transformer(x)\n",
        "        return self.cls_head(x)\n"
      ],
      "metadata": {
        "id": "nfj8KjBNULiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# config\n",
        "BEST_CONFIG = {\n",
        "    'sequence_length': 16,\n",
        "    'dropout': 0.4,\n",
        "    'hidden_dim': 128,\n",
        "    'num_layers': 1,\n",
        "    'lr': 0.0001\n",
        "}\n",
        "\n",
        "wandb.init(project=\"video-anomaly-detection\", name=\"final-dinov2-multiseq-smoothing\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = FinalVideoTransformer2(\n",
        "    feature_dim=768,\n",
        "    num_classes=len(CLASS_NAMES),\n",
        "    sequence_length=BEST_CONFIG['sequence_length'],\n",
        "    dropout=BEST_CONFIG['dropout'],\n",
        "    hidden_dim=BEST_CONFIG['hidden_dim'],\n",
        "    num_layers=BEST_CONFIG['num_layers']\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=BEST_CONFIG['lr'], weight_decay=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)  # label smoothing added\n",
        "\n",
        "print(\"\\ntraining begins\\n\")\n",
        "\n",
        "for epoch in range(30):\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "    train_loss, train_preds, train_targets = 0, [], []\n",
        "\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_x)\n",
        "        loss = loss_fn(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * batch_x.size(0)\n",
        "        train_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "        train_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    train_acc = accuracy_score(train_targets, train_preds)\n",
        "    train_auc = roc_auc_score(\n",
        "        F.one_hot(torch.tensor(train_targets), num_classes=len(CLASS_NAMES)),\n",
        "        F.one_hot(torch.tensor(train_preds), num_classes=len(CLASS_NAMES)),\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    test_loss, test_preds, test_targets = 0, [], []\n",
        "    with torch.no_grad():\n",
        "        for batch_x, batch_y in test_loader:\n",
        "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
        "            outputs = model(batch_x)\n",
        "            loss = loss_fn(outputs, batch_y)\n",
        "            test_loss += loss.item() * batch_x.size(0)\n",
        "            test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            test_targets.extend(batch_y.cpu().numpy())\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_acc = accuracy_score(test_targets, test_preds)\n",
        "    test_auc = roc_auc_score(\n",
        "        F.one_hot(torch.tensor(test_targets), num_classes=len(CLASS_NAMES)),\n",
        "        F.one_hot(torch.tensor(test_preds), num_classes=len(CLASS_NAMES)),\n",
        "        multi_class='ovr'\n",
        "    )\n",
        "\n",
        "    print(f\"[epoch {epoch+1:02d}] train loss: {train_loss:.4f}, acc: {train_acc:.4f}, auc: {train_auc:.4f} | \"\n",
        "          f\"test loss: {test_loss:.4f}, acc: {test_acc:.4f}, auc: {test_auc:.4f}\")\n",
        "\n",
        "    wandb.log({\n",
        "        'epoch': epoch + 1,\n",
        "        'train_loss': train_loss,\n",
        "        'train_accuracy': train_acc,\n",
        "        'train_auc': train_auc,\n",
        "        'test_loss': test_loss,\n",
        "        'test_accuracy': test_acc,\n",
        "        'test_auc': test_auc\n",
        "    })\n",
        "\n",
        "torch.save(model.state_dict(), \"final_best_model_dinov2_multiseq.pt\")\n",
        "print(f\"epoch {epoch+1} take {time.time() - start_time:.2f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 749
        },
        "id": "9a_cNHQCLn1W",
        "outputId": "ecf9aafe-0a73-4c95-8ced-2bc6148feabb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250425_142422-q07bq7t4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/marium-waseem-city-university-of-london/video-anomaly-detection/runs/q07bq7t4' target=\"_blank\">final-dinov2-multiseq-smoothing</a></strong> to <a href='https://wandb.ai/marium-waseem-city-university-of-london/video-anomaly-detection' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/marium-waseem-city-university-of-london/video-anomaly-detection' target=\"_blank\">https://wandb.ai/marium-waseem-city-university-of-london/video-anomaly-detection</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/marium-waseem-city-university-of-london/video-anomaly-detection/runs/q07bq7t4' target=\"_blank\">https://wandb.ai/marium-waseem-city-university-of-london/video-anomaly-detection/runs/q07bq7t4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "training begins\n",
            "\n",
            "[epoch 01] train loss: 1.5675, acc: 0.4836, auc: 0.6617 | test loss: 1.8586, acc: 0.4020, auc: 0.6359\n",
            "[epoch 02] train loss: 1.0247, acc: 0.7740, auc: 0.8479 | test loss: 2.3584, acc: 0.2900, auc: 0.5814\n",
            "[epoch 03] train loss: 0.7675, acc: 0.9111, auc: 0.9412 | test loss: 2.6803, acc: 0.2998, auc: 0.5870\n",
            "[epoch 04] train loss: 0.6688, acc: 0.9644, auc: 0.9771 | test loss: 2.3395, acc: 0.3476, auc: 0.6146\n",
            "[epoch 05] train loss: 0.6188, acc: 0.9842, auc: 0.9895 | test loss: 2.3125, acc: 0.3558, auc: 0.6191\n",
            "[epoch 06] train loss: 0.5905, acc: 0.9933, auc: 0.9949 | test loss: 2.3830, acc: 0.3460, auc: 0.6168\n",
            "[epoch 07] train loss: 0.5774, acc: 0.9957, auc: 0.9968 | test loss: 2.3900, acc: 0.3410, auc: 0.6045\n",
            "[epoch 08] train loss: 0.5720, acc: 0.9941, auc: 0.9958 | test loss: 2.4604, acc: 0.3427, auc: 0.6027\n",
            "[epoch 09] train loss: 0.5685, acc: 0.9957, auc: 0.9972 | test loss: 2.3812, acc: 0.3328, auc: 0.6016\n",
            "[epoch 10] train loss: 0.5519, acc: 0.9988, auc: 0.9994 | test loss: 2.4121, acc: 0.3427, auc: 0.6137\n",
            "[epoch 11] train loss: 0.5503, acc: 1.0000, auc: 1.0000 | test loss: 2.3455, acc: 0.3394, auc: 0.6084\n",
            "[epoch 12] train loss: 0.5476, acc: 0.9992, auc: 0.9994 | test loss: 2.3518, acc: 0.3773, auc: 0.6273\n",
            "[epoch 13] train loss: 0.5428, acc: 0.9980, auc: 0.9985 | test loss: 2.5176, acc: 0.3509, auc: 0.6058\n",
            "[epoch 14] train loss: 0.5420, acc: 0.9996, auc: 0.9998 | test loss: 2.4991, acc: 0.3493, auc: 0.6174\n",
            "[epoch 15] train loss: 0.5343, acc: 1.0000, auc: 1.0000 | test loss: 2.5179, acc: 0.3328, auc: 0.5950\n",
            "[epoch 16] train loss: 0.5327, acc: 1.0000, auc: 1.0000 | test loss: 2.4670, acc: 0.3361, auc: 0.6034\n",
            "[epoch 17] train loss: 0.5339, acc: 1.0000, auc: 1.0000 | test loss: 2.4797, acc: 0.3427, auc: 0.6131\n",
            "[epoch 18] train loss: 0.5393, acc: 0.9972, auc: 0.9985 | test loss: 2.6206, acc: 0.3344, auc: 0.6091\n",
            "[epoch 19] train loss: 0.5337, acc: 1.0000, auc: 1.0000 | test loss: 2.4938, acc: 0.3542, auc: 0.6176\n",
            "[epoch 20] train loss: 0.5234, acc: 1.0000, auc: 1.0000 | test loss: 2.6204, acc: 0.3591, auc: 0.6193\n",
            "[epoch 21] train loss: 0.5229, acc: 1.0000, auc: 1.0000 | test loss: 2.5868, acc: 0.3245, auc: 0.6050\n",
            "[epoch 22] train loss: 0.5237, acc: 0.9996, auc: 0.9998 | test loss: 2.5477, acc: 0.3608, auc: 0.6199\n",
            "[epoch 23] train loss: 0.5190, acc: 1.0000, auc: 1.0000 | test loss: 2.5074, acc: 0.3542, auc: 0.6192\n",
            "[epoch 24] train loss: 0.5277, acc: 0.9960, auc: 0.9976 | test loss: 2.8439, acc: 0.3213, auc: 0.5949\n",
            "[epoch 25] train loss: 0.5269, acc: 0.9976, auc: 0.9985 | test loss: 2.5601, acc: 0.3394, auc: 0.6095\n",
            "[epoch 26] train loss: 0.5176, acc: 1.0000, auc: 1.0000 | test loss: 2.6018, acc: 0.3575, auc: 0.6192\n",
            "[epoch 27] train loss: 0.5192, acc: 0.9980, auc: 0.9988 | test loss: 2.5554, acc: 0.3328, auc: 0.6041\n",
            "[epoch 28] train loss: 0.5294, acc: 0.9976, auc: 0.9987 | test loss: 2.7535, acc: 0.3295, auc: 0.6051\n",
            "[epoch 29] train loss: 0.5135, acc: 1.0000, auc: 1.0000 | test loss: 2.7831, acc: 0.3213, auc: 0.6054\n",
            "[epoch 30] train loss: 0.5135, acc: 1.0000, auc: 1.0000 | test loss: 2.7001, acc: 0.3361, auc: 0.6005\n",
            "epoch 30 take 365.08 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qpXi048LnxN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNu1aOpBZn9fEiMIx7tSZgc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d5129a04689f43a8b0d400113f8f1a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d5da6f1334f4d77ba9e69b089cce178",
              "IPY_MODEL_debc7d87b4104de2b5fb9984b637a70b",
              "IPY_MODEL_0deb54dd23614de7afeed6ca5cb73bd5"
            ],
            "layout": "IPY_MODEL_1cc740f67c9449b09926f51caf8b791d"
          }
        },
        "2d5da6f1334f4d77ba9e69b089cce178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bf3cfb98bab40cf871e706032b629ea",
            "placeholder": "​",
            "style": "IPY_MODEL_9ff6ab368a7e44609b3b3843993ca3cf",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "debc7d87b4104de2b5fb9984b637a70b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d3b9372d2844b73a0f1dbc686946103",
            "max": 436,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_21e19e717f724376a1b912b84917120e",
            "value": 436
          }
        },
        "0deb54dd23614de7afeed6ca5cb73bd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7808eeecb2ab441cb46d83ba6499c3f8",
            "placeholder": "​",
            "style": "IPY_MODEL_3ad04e9c85d543eca664a861e0a79ed2",
            "value": " 436/436 [00:00&lt;00:00, 52.3kB/s]"
          }
        },
        "1cc740f67c9449b09926f51caf8b791d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf3cfb98bab40cf871e706032b629ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ff6ab368a7e44609b3b3843993ca3cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d3b9372d2844b73a0f1dbc686946103": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21e19e717f724376a1b912b84917120e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7808eeecb2ab441cb46d83ba6499c3f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ad04e9c85d543eca664a861e0a79ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "580ac6e13534478bb9a8132450a225ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2194986a52c4d8a88fccbf1da7500ad",
              "IPY_MODEL_7ec600f23c5d460194e0df9308638ac9",
              "IPY_MODEL_3c163a8e698a4b0a9ea6a9d19b4a2e94"
            ],
            "layout": "IPY_MODEL_2098129901ed41beac8cc00d6a44d4ec"
          }
        },
        "b2194986a52c4d8a88fccbf1da7500ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cea5e40ddae145d3a72ac5be61ea5f34",
            "placeholder": "​",
            "style": "IPY_MODEL_773d6bce56304e04acddd7457c2b08c5",
            "value": "config.json: 100%"
          }
        },
        "7ec600f23c5d460194e0df9308638ac9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d29f92bf0c34be7bf4a2256ccfc6a08",
            "max": 548,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e05bec7685614add914dadfa6ce51f15",
            "value": 548
          }
        },
        "3c163a8e698a4b0a9ea6a9d19b4a2e94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_369168e857e343ddbfc403f76ef8ac3e",
            "placeholder": "​",
            "style": "IPY_MODEL_7c734beb073b4e8fa12c545de7e6164e",
            "value": " 548/548 [00:00&lt;00:00, 70.4kB/s]"
          }
        },
        "2098129901ed41beac8cc00d6a44d4ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea5e40ddae145d3a72ac5be61ea5f34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "773d6bce56304e04acddd7457c2b08c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d29f92bf0c34be7bf4a2256ccfc6a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e05bec7685614add914dadfa6ce51f15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "369168e857e343ddbfc403f76ef8ac3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c734beb073b4e8fa12c545de7e6164e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef1a6ef21b964f6285e68d582703dafb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ea907ea17cb40adbcb71f4bb187a7f7",
              "IPY_MODEL_049de2b16baa42d29e67f0aba40f3969",
              "IPY_MODEL_e8068aa8484f4b49bb236823cef0a1b1"
            ],
            "layout": "IPY_MODEL_0708d7f480a2455898c2d93018c97f45"
          }
        },
        "2ea907ea17cb40adbcb71f4bb187a7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18b5fe694d264766aa2c953481e0314d",
            "placeholder": "​",
            "style": "IPY_MODEL_b24b4db958224668a5db058f8789025f",
            "value": "model.safetensors: 100%"
          }
        },
        "049de2b16baa42d29e67f0aba40f3969": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a13062537cd84ef08b809abe836e8a3b",
            "max": 346345912,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d6210cb69314f80ada174be5d85feea",
            "value": 346345912
          }
        },
        "e8068aa8484f4b49bb236823cef0a1b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9900debefe054250b04b1f4752b95141",
            "placeholder": "​",
            "style": "IPY_MODEL_53f2ea261cc04ef3bf05c17898e0cf4e",
            "value": " 346M/346M [00:01&lt;00:00, 275MB/s]"
          }
        },
        "0708d7f480a2455898c2d93018c97f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18b5fe694d264766aa2c953481e0314d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b24b4db958224668a5db058f8789025f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a13062537cd84ef08b809abe836e8a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d6210cb69314f80ada174be5d85feea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9900debefe054250b04b1f4752b95141": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f2ea261cc04ef3bf05c17898e0cf4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}